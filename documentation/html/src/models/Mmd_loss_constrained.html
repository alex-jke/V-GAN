<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>src.models.Mmd_loss_constrained API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.models.Mmd_loss_constrained</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.models.Mmd_loss_constrained.EfficientRBF"><code class="flex name class">
<span>class <span class="ident">EfficientRBF</span></span>
<span>(</span><span>n_kernels: int = 5,<br>mul_factor: float = 2.0,<br>embedding=&lt;function EfficientRBF.&lt;lambda&gt;&gt;)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EfficientRBF(nn.Module):
    def __init__(self,
                 n_kernels: int = 5,
                 mul_factor: float = 2.0,
                 embedding=lambda x: x):
        super().__init__()
        self.device = torch.device(&#39;cuda&#39; if torch.cuda.is_available(
        ) else &#39;mps&#39; if torch.backends.mps.is_available() else &#39;cpu&#39;)
        self.n_kernels = n_kernels
        self.embedding = embedding
        # μ_i = mul_factor ** (i - n_kernels//2) for i∈[0..K)
        bw_mult = mul_factor ** (torch.arange(n_kernels) - n_kernels//2)
        self.bandwidth_multipliers = bw_mult
        self.register_buffer(&#34;bw_mult&#34;, bw_mult.float())

    def forward(self, X: torch.Tensor, Y: torch.Tensor):
        &#34;&#34;&#34;
        X, Y: (B, D)
        Returns three (B,B) blocks K_xx, K_xy, K_yy that match the original RBF.
        &#34;&#34;&#34;
        X = self.embedding(X)
        Y = self.embedding(Y).to(X.device)
        B = X.size(0)

        # squared norms
        x_norm = (X*X).sum(1, keepdim=True)   # (B,1)
        y_norm = (Y*Y).sum(1, keepdim=True)   # (B,1)

        # pairwise squared distances
        D_xx = x_norm + x_norm.t() - 2*(X @ X.t())
        D_yy = y_norm + y_norm.t() - 2*(Y @ Y.t())
        D_xy = x_norm + y_norm.t() - 2*(X @ Y.t())

        # build the off-diag bandwidth pool exactly like the original
        mask = ~torch.eye(B, device=X.device, dtype=torch.bool)
        flat_xx = D_xx[mask]            # B^2 – B entries
        flat_yy = D_yy[mask]
        flat_xy = D_xy.reshape(-1)      # B^2 entries
        # include both X-&gt;Y and Y-&gt;X just once each
        all_offdiag = torch.cat([
            flat_xx,
            flat_yy,
            flat_xy,
            flat_xy  # duplicate for Y-&gt;X
        ], dim=0)

        # exact same denominator (4B^2 – 2B)
        base_bw = all_offdiag.sum() / all_offdiag.numel()
        self.bandwidth = base_bw
        if base_bw.device != self.bw_mult.device:
            base_bw = base_bw.to(self.bw_mult.device)
        bws = base_bw * self.bw_mult  # (K,)

        # accumulate sum over kernels K)
        K_xx = torch.zeros_like(D_xx)
        K_xy = torch.zeros_like(D_xy)
        K_yy = torch.zeros_like(D_yy)
        for σ2 in bws:
            inv = 1.0/σ2
            K_xx += torch.exp(-D_xx * inv)
            K_xy += torch.exp(-D_xy * inv)
            K_yy += torch.exp(-D_yy * inv)

        return K_xx, K_xy, K_yy</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.models.Mmd_loss_constrained.EfficientRBF.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, X: torch.Tensor, Y: torch.Tensor) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, X: torch.Tensor, Y: torch.Tensor):
    &#34;&#34;&#34;
    X, Y: (B, D)
    Returns three (B,B) blocks K_xx, K_xy, K_yy that match the original RBF.
    &#34;&#34;&#34;
    X = self.embedding(X)
    Y = self.embedding(Y).to(X.device)
    B = X.size(0)

    # squared norms
    x_norm = (X*X).sum(1, keepdim=True)   # (B,1)
    y_norm = (Y*Y).sum(1, keepdim=True)   # (B,1)

    # pairwise squared distances
    D_xx = x_norm + x_norm.t() - 2*(X @ X.t())
    D_yy = y_norm + y_norm.t() - 2*(Y @ Y.t())
    D_xy = x_norm + y_norm.t() - 2*(X @ Y.t())

    # build the off-diag bandwidth pool exactly like the original
    mask = ~torch.eye(B, device=X.device, dtype=torch.bool)
    flat_xx = D_xx[mask]            # B^2 – B entries
    flat_yy = D_yy[mask]
    flat_xy = D_xy.reshape(-1)      # B^2 entries
    # include both X-&gt;Y and Y-&gt;X just once each
    all_offdiag = torch.cat([
        flat_xx,
        flat_yy,
        flat_xy,
        flat_xy  # duplicate for Y-&gt;X
    ], dim=0)

    # exact same denominator (4B^2 – 2B)
    base_bw = all_offdiag.sum() / all_offdiag.numel()
    self.bandwidth = base_bw
    if base_bw.device != self.bw_mult.device:
        base_bw = base_bw.to(self.bw_mult.device)
    bws = base_bw * self.bw_mult  # (K,)

    # accumulate sum over kernels K)
    K_xx = torch.zeros_like(D_xx)
    K_xy = torch.zeros_like(D_xy)
    K_yy = torch.zeros_like(D_yy)
    for σ2 in bws:
        inv = 1.0/σ2
        K_xx += torch.exp(-D_xx * inv)
        K_xy += torch.exp(-D_xy * inv)
        K_yy += torch.exp(-D_yy * inv)

    return K_xx, K_xy, K_yy</code></pre>
</details>
<div class="desc"><p>X, Y: (B, D)
Returns three (B,B) blocks K_xx, K_xy, K_yy that match the original RBF.</p></div>
</dd>
</dl>
</dd>
<dt id="src.models.Mmd_loss_constrained.MMDLossConstrained"><code class="flex name class">
<span>class <span class="ident">MMDLossConstrained</span></span>
<span>(</span><span>weight, kernel=EfficientRBF(), subspace_amount_penalty=3, middle_penalty=None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MMDLossConstrained(nn.Module):
    &#39;&#39;&#39;
    Constrained loss by the number of features selected
    &#39;&#39;&#39;

    def __init__(self, weight, kernel=EfficientRBF(), subspace_amount_penalty = 3, middle_penalty = None):
        super().__init__()
        self.kernel = kernel
        self.weight = weight
        self.device = torch.device(&#39;cuda&#39; if torch.cuda.is_available(
        ) else &#39;mps:0&#39; if torch.backends.mps.is_available() else &#39;cpu&#39;)
        self.subspace_penalty = subspace_amount_penalty
        self.middle_penalty = middle_penalty
        if self.middle_penalty is None:
            self.middle_penalty = 0.0

    @staticmethod
    def diversity_loss(M, tau=0.1):
        # M: shape (n, d) with entries in [0,1]
        dist = torch.cdist(M, M) ** 2  # (n, n) squared distances
        sim = torch.exp(-dist / tau)  # similarity: high if rows are similar
        # Zero out self-similarity
        sim = sim - torch.diag_embed(sim.diagonal(0))
        return sim.mean()  # Lower loss =&gt; higher diversity

    def get_loss(self, X, Y, U, apply_penalty = True):
        #K = self.kernel(torch.vstack([X, Y]))
        #K = self.kernel(X, Y)

        #X_size = X.shape[0]
        #XX = K[:X_size, :X_size].mean()
        #XY = K[:X_size, X_size:].mean()
        #YY = K[X_size:, X_size:].mean()

        U = U.to(X.device)


        #kernel_eff = EfficientRBF()
        K_xx, K_xy, K_yy = self.kernel(X, Y)
        self.bandwidth = self.kernel.bandwidth
        self.bandwidth_multipliers = self.kernel.bandwidth_multipliers
        XX= K_xx.mean()
        XY = K_xy.mean()
        YY = K_yy.mean()

        #assert torch.allclose(XX, XX_eff) and torch.allclose(XY, XY_eff) and torch.allclose(YY, YY_eff)

        if torch.isnan(XX).any() or torch.isnan(XY).any() or torch.isnan(YY).any():
            raise ValueError(&#34;XX or XY contain nan values.&#34;)
            XX = torch.where(torch.isnan(XX), torch.zeros_like(XX), XX)
            XY = torch.where(torch.isnan(XY), torch.zeros_like(XY), XY)
            YY = torch.where(torch.isnan(YY), torch.zeros_like(YY), YY)

        # ones = torch.ones(U.shape[1]).to(self.device)
        # topk = torch.topk(U, 10, 0).values.float().mean(dim=0)
        mean = U.float().mean(dim=0)
        avg = mean.sum() / U.shape[1]
        #zero = torch.zeros_like(mean)
        #feature_selection_penalty = (torch.less_equal(mean,zero)* 1.0).mean() if apply_penalty else 0
        # avg_u = U.float().mean(dim=0)
        # mean = torch.mean(ones - topk)
        # penalty = self.weight * (mean)
        #penalty = self.weight * (avg) if apply_penalty else 0 # self.weight*(mean)
        penalty = 0
        diversity_loss = 0
        if apply_penalty or self.weight != 0.0:
            penalty = self.weight * avg #torch.exp(avg)  # self.weight*(mean)
            #diversity_loss = self.diversity_loss(U.float())
        #u_sizes = U.float().sum(dim=1)
        #median = u_sizes.median()
        #penalty = self.weight * (median) if  apply_penalty else 0

        # middle penalty to punish the generator for generating subspaces with prob close to 0.5
        #middle_matrix = (-U.float() * (U.float() - 1))
        #middle_penalty = middle_matrix.mean(dim=0).sum() / U.shape[1] if apply_penalty else 0
        #middle_penalty *= self.middle_penalty

        mmd_loss = XX - 2 * XY + YY
        if math.isnan(mmd_loss):
            raise ValueError(&#34;mmd is nan.&#34;)
        return (mmd_loss +
                penalty
                 + diversity_loss# + middle_penalty# + feature_selection_penalty
                , mmd_loss)

    def forward(self, X, Y, U: torch.Tensor, apply_penalty = True):
        full_loss, self.mmd_loss = self.get_loss(X, Y, U, apply_penalty)
        return full_loss</code></pre>
</details>
<div class="desc"><p>Constrained loss by the number of features selected</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="src.models.Mmd_loss_constrained.MMDLossConstrained.diversity_loss"><code class="name flex">
<span>def <span class="ident">diversity_loss</span></span>(<span>M, tau=0.1)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def diversity_loss(M, tau=0.1):
    # M: shape (n, d) with entries in [0,1]
    dist = torch.cdist(M, M) ** 2  # (n, n) squared distances
    sim = torch.exp(-dist / tau)  # similarity: high if rows are similar
    # Zero out self-similarity
    sim = sim - torch.diag_embed(sim.diagonal(0))
    return sim.mean()  # Lower loss =&gt; higher diversity</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="src.models.Mmd_loss_constrained.MMDLossConstrained.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, X, Y, U: torch.Tensor, apply_penalty=True) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, X, Y, U: torch.Tensor, apply_penalty = True):
    full_loss, self.mmd_loss = self.get_loss(X, Y, U, apply_penalty)
    return full_loss</code></pre>
</details>
<div class="desc"><p>Define the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
</dd>
<dt id="src.models.Mmd_loss_constrained.MMDLossConstrained.get_loss"><code class="name flex">
<span>def <span class="ident">get_loss</span></span>(<span>self, X, Y, U, apply_penalty=True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_loss(self, X, Y, U, apply_penalty = True):
    #K = self.kernel(torch.vstack([X, Y]))
    #K = self.kernel(X, Y)

    #X_size = X.shape[0]
    #XX = K[:X_size, :X_size].mean()
    #XY = K[:X_size, X_size:].mean()
    #YY = K[X_size:, X_size:].mean()

    U = U.to(X.device)


    #kernel_eff = EfficientRBF()
    K_xx, K_xy, K_yy = self.kernel(X, Y)
    self.bandwidth = self.kernel.bandwidth
    self.bandwidth_multipliers = self.kernel.bandwidth_multipliers
    XX= K_xx.mean()
    XY = K_xy.mean()
    YY = K_yy.mean()

    #assert torch.allclose(XX, XX_eff) and torch.allclose(XY, XY_eff) and torch.allclose(YY, YY_eff)

    if torch.isnan(XX).any() or torch.isnan(XY).any() or torch.isnan(YY).any():
        raise ValueError(&#34;XX or XY contain nan values.&#34;)
        XX = torch.where(torch.isnan(XX), torch.zeros_like(XX), XX)
        XY = torch.where(torch.isnan(XY), torch.zeros_like(XY), XY)
        YY = torch.where(torch.isnan(YY), torch.zeros_like(YY), YY)

    # ones = torch.ones(U.shape[1]).to(self.device)
    # topk = torch.topk(U, 10, 0).values.float().mean(dim=0)
    mean = U.float().mean(dim=0)
    avg = mean.sum() / U.shape[1]
    #zero = torch.zeros_like(mean)
    #feature_selection_penalty = (torch.less_equal(mean,zero)* 1.0).mean() if apply_penalty else 0
    # avg_u = U.float().mean(dim=0)
    # mean = torch.mean(ones - topk)
    # penalty = self.weight * (mean)
    #penalty = self.weight * (avg) if apply_penalty else 0 # self.weight*(mean)
    penalty = 0
    diversity_loss = 0
    if apply_penalty or self.weight != 0.0:
        penalty = self.weight * avg #torch.exp(avg)  # self.weight*(mean)
        #diversity_loss = self.diversity_loss(U.float())
    #u_sizes = U.float().sum(dim=1)
    #median = u_sizes.median()
    #penalty = self.weight * (median) if  apply_penalty else 0

    # middle penalty to punish the generator for generating subspaces with prob close to 0.5
    #middle_matrix = (-U.float() * (U.float() - 1))
    #middle_penalty = middle_matrix.mean(dim=0).sum() / U.shape[1] if apply_penalty else 0
    #middle_penalty *= self.middle_penalty

    mmd_loss = XX - 2 * XY + YY
    if math.isnan(mmd_loss):
        raise ValueError(&#34;mmd is nan.&#34;)
    return (mmd_loss +
            penalty
             + diversity_loss# + middle_penalty# + feature_selection_penalty
            , mmd_loss)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="src.models.Mmd_loss_constrained.MixtureRQLinear"><code class="flex name class">
<span>class <span class="ident">MixtureRQLinear</span></span>
<span>(</span><span>alphas=None, linear_weight=1.0)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MixtureRQLinear(nn.Module):

    def get_bandwidth(self, L2_distances):
        n_samples = L2_distances.shape[0]
        self.bandwidth = L2_distances.data.sum() / (n_samples ** 2 - n_samples)
        return L2_distances.data.sum() / (n_samples ** 2 - n_samples)

    def __init__(self, alphas=None, linear_weight=1.0):
        &#34;&#34;&#34;
        Mixture of Rational Quadratic Kernels with a Linear Kernel.

        Args:
            alphas (list of float): List of alpha values for the RQ kernels.
                                   Each alpha corresponds to a different RQ kernel.
            linear_weight (float): Weight for the linear kernel.
        &#34;&#34;&#34;
        super().__init__()
        if alphas is None:
            alphas = [0.2, 0.5, 1.0, 2.0, 5]
        self.alphas = alphas
        self.linear_weight = linear_weight

    def forward(self, X: torch.Tensor, Y: torch.Tensor):
        &#34;&#34;&#34;
        Compute the mixture of RQ kernels and linear kernel.

        Args:
            X (torch.Tensor): Input tensor of shape (n_samples, n_features).
            Y (torch.Tensor): Input tensor of shape (n_samples, n_features).

        Returns:
            torch.Tensor: Combined kernel matrix of shape (n_samples, n_samples).
        &#34;&#34;&#34;
        stacked = torch.vstack([X, Y])
        L2_distances = torch.cdist(stacked, stacked) ** 2
        self.bandwidth = self.get_bandwidth(L2_distances)

        rq_kernels = []
        for alpha in self.alphas:
            rq_kernel = (1 + L2_distances / (2 * alpha)) ** (-alpha)
            rq_kernels.append(rq_kernel)

        rq_mixture = torch.sum(torch.stack(rq_kernels), dim=0)

        linear_kernel = torch.matmul(stacked, stacked.T)

        combined_kernel = rq_mixture + self.linear_weight * linear_kernel

        return combined_kernel</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Mixture of Rational Quadratic Kernels with a Linear Kernel.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>alphas</code></strong> :&ensp;<code>list</code> of <code>float</code></dt>
<dd>List of alpha values for the RQ kernels.
Each alpha corresponds to a different RQ kernel.</dd>
<dt><strong><code>linear_weight</code></strong> :&ensp;<code>float</code></dt>
<dd>Weight for the linear kernel.</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.models.Mmd_loss_constrained.MixtureRQLinear.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, X: torch.Tensor, Y: torch.Tensor) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, X: torch.Tensor, Y: torch.Tensor):
    &#34;&#34;&#34;
    Compute the mixture of RQ kernels and linear kernel.

    Args:
        X (torch.Tensor): Input tensor of shape (n_samples, n_features).
        Y (torch.Tensor): Input tensor of shape (n_samples, n_features).

    Returns:
        torch.Tensor: Combined kernel matrix of shape (n_samples, n_samples).
    &#34;&#34;&#34;
    stacked = torch.vstack([X, Y])
    L2_distances = torch.cdist(stacked, stacked) ** 2
    self.bandwidth = self.get_bandwidth(L2_distances)

    rq_kernels = []
    for alpha in self.alphas:
        rq_kernel = (1 + L2_distances / (2 * alpha)) ** (-alpha)
        rq_kernels.append(rq_kernel)

    rq_mixture = torch.sum(torch.stack(rq_kernels), dim=0)

    linear_kernel = torch.matmul(stacked, stacked.T)

    combined_kernel = rq_mixture + self.linear_weight * linear_kernel

    return combined_kernel</code></pre>
</details>
<div class="desc"><p>Compute the mixture of RQ kernels and linear kernel.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Input tensor of shape (n_samples, n_features).</dd>
<dt><strong><code>Y</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Input tensor of shape (n_samples, n_features).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Combined kernel matrix of shape (n_samples, n_samples).</dd>
</dl></div>
</dd>
<dt id="src.models.Mmd_loss_constrained.MixtureRQLinear.get_bandwidth"><code class="name flex">
<span>def <span class="ident">get_bandwidth</span></span>(<span>self, L2_distances)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_bandwidth(self, L2_distances):
    n_samples = L2_distances.shape[0]
    self.bandwidth = L2_distances.data.sum() / (n_samples ** 2 - n_samples)
    return L2_distances.data.sum() / (n_samples ** 2 - n_samples)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="src.models.Mmd_loss_constrained.RBF"><code class="flex name class">
<span>class <span class="ident">RBF</span></span>
<span>(</span><span>n_kernels=5,<br>mul_factor=2.0,<br>bandwidth=None,<br>embedding=&lt;function RBF.&lt;lambda&gt;&gt;,<br>vector_norm: models.norm.VectorNorm = &lt;models.norm.L2Norm object&gt;,<br>matrix_norm: models.norm.MatrixNorm = &lt;models.norm.FrobeniusNorm object&gt;)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RBF(nn.Module):

    def __init__(self, n_kernels=5, mul_factor=2.0, bandwidth=None, embedding = lambda x: x,
                 vector_norm: VectorNorm = L2Norm(),
                 matrix_norm: MatrixNorm = FrobeniusNorm()):
        super().__init__()
        device = torch.device(&#39;cuda&#39; if torch.cuda.is_available(
        ) else &#39;mps:0&#39; if torch.backends.mps.is_available() else &#39;cpu&#39;)

        self.bandwidth_multipliers = mul_factor ** (
            torch.arange(n_kernels) - n_kernels // 2).to(device)
        self.bandwidth = bandwidth
        self.embedding = embedding
        self.vector_norm = vector_norm
        self.matrix_norm = matrix_norm

    def get_bandwidth(self, L2_distances):
        n_samples = L2_distances.shape[0]
        self.bandwidth = L2_distances.data.sum() / (n_samples ** 2 - n_samples)
        return self.bandwidth

    &#34;&#34;&#34;def get_bandwidth(self, L2_distances):
        # Flatten and compute median of non-zero distances
        flat_dist = L2_distances.flatten()
        non_zero = flat_dist[flat_dist &gt; 0]
        self.bandwidth = torch.median(non_zero)
        return self.bandwidth&#34;&#34;&#34;

    def forward(self, X: torch.Tensor, Y: torch.Tensor):
        &#39;&#39;&#39;
        X: torch.Tensor
            The input tensor of shape (n_samples, feature_dim * 2) (X and Y are concatenated)
            Alternatively, a shape of (embedding_dim, n_samples, feature_dim)
        &#39;&#39;&#39;
        X_embedded = self.embedding(X)
        Y_embedded = self.embedding(Y)

        embedded = torch.vstack([X_embedded, Y_embedded])
        if len(embedded.shape) == SHAPE_LEN_VECTOR:
            norm = self.vector_norm.compute_distance_matrix
        elif len(embedded.shape) == SHAPE_LEN_MATRIX:
            norm = self.matrix_norm.compute_distance_matrix
        else:
            raise ValueError(&#34;Input Tensor has to either be of shape (BxN) or (ExBxN)&#34;)

        distances = norm(embedded)

        squared_distances = distances ** 2

        if torch.isnan(squared_distances).any():
            print(&#34;L2 distances are nan.&#34;)

        result = self._compute_rbf_kernel(squared_distances)

        if torch.isnan(result).any():
            print(&#34;result are nan.&#34;)
        return result

    def _compute_rbf_kernel(self, L2_distances):
        &#34;&#34;&#34;
        This method also works for L2_distance tensors of three dimensions. Expected, however, is a two-dimensional
        tensor of size (2Bx2B)
        &#34;&#34;&#34;
        # Get bandwidth value (single scalar) based on the input L2_distances
        base_bandwidth = self.get_bandwidth(L2_distances)  # scalar

        # Multiply by bandwidth multipliers to get multiple bandwidths
        # Shape: (n_kernels,)
        bandwidths = base_bandwidth * self.bandwidth_multipliers

        # Reshape bandwidths for broadcasting
        # Shape: (n_kernels, 1, 1, 1) to match (n_kernels, number_samples, feature_space_dimension, embedding_dimension)
        bandwidths_padded = bandwidths[:, None, None, None]

        # Reshape L2_distances for broadcasting
        # Shape: (1, number_samples, feature_space_dimension, embedding_dimension)
        L2_distances = L2_distances[None, ...]

        # Compute the RBF kernel for each bandwidth
        # Exponential part of RBF kernel: exp(- L2_distances / (2 * bandwidths**2))
        result_full = torch.exp(-L2_distances / (bandwidths_padded))
        result = result_full.sum(dim=0)

        # Sum over the embeddings dimension
        result_reduced = result.mean(dim=0) # check if this still is a norm. Try to turn it into a norm.

        if torch.isnan(result_reduced).any():
            print(&#34;kernel produced nan value&#34;)

        # Result should be of shape (2*batch_size, 2*batch_size)
        return result_reduced</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.models.Mmd_loss_constrained.RBF.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, X: torch.Tensor, Y: torch.Tensor) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, X: torch.Tensor, Y: torch.Tensor):
    &#39;&#39;&#39;
    X: torch.Tensor
        The input tensor of shape (n_samples, feature_dim * 2) (X and Y are concatenated)
        Alternatively, a shape of (embedding_dim, n_samples, feature_dim)
    &#39;&#39;&#39;
    X_embedded = self.embedding(X)
    Y_embedded = self.embedding(Y)

    embedded = torch.vstack([X_embedded, Y_embedded])
    if len(embedded.shape) == SHAPE_LEN_VECTOR:
        norm = self.vector_norm.compute_distance_matrix
    elif len(embedded.shape) == SHAPE_LEN_MATRIX:
        norm = self.matrix_norm.compute_distance_matrix
    else:
        raise ValueError(&#34;Input Tensor has to either be of shape (BxN) or (ExBxN)&#34;)

    distances = norm(embedded)

    squared_distances = distances ** 2

    if torch.isnan(squared_distances).any():
        print(&#34;L2 distances are nan.&#34;)

    result = self._compute_rbf_kernel(squared_distances)

    if torch.isnan(result).any():
        print(&#34;result are nan.&#34;)
    return result</code></pre>
</details>
<div class="desc"><p>X: torch.Tensor
The input tensor of shape (n_samples, feature_dim * 2) (X and Y are concatenated)
Alternatively, a shape of (embedding_dim, n_samples, feature_dim)</p></div>
</dd>
<dt id="src.models.Mmd_loss_constrained.RBF.get_bandwidth"><code class="name flex">
<span>def <span class="ident">get_bandwidth</span></span>(<span>self, L2_distances)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_bandwidth(self, L2_distances):
    n_samples = L2_distances.shape[0]
    self.bandwidth = L2_distances.data.sum() / (n_samples ** 2 - n_samples)
    return self.bandwidth</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="src.models.Mmd_loss_constrained.RationalQuadratic"><code class="flex name class">
<span>class <span class="ident">RationalQuadratic</span></span>
<span>(</span><span>alpha=0.5)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RationalQuadratic(nn.Module):
    def __init__(self, alpha=0.5):
        &#34;&#34;&#34;
        Rational Quadratic Kernel.

        Args:
            alpha (float): Shape parameter. Controls the tail behavior of the kernel.
                           Higher values make the kernel behave more like the RBF kernel.
        &#34;&#34;&#34;
        super().__init__()
        self.alpha = alpha

    def get_bandwidth(self, L2_distances):
        n_samples = L2_distances.shape[0]
        self.bandwidth = L2_distances.data.sum() / (n_samples ** 2 - n_samples)
        return L2_distances.data.sum() / (n_samples ** 2 - n_samples)

    def forward(self, X):
        &#34;&#34;&#34;
        Compute the Rational Quadratic Kernel matrix.

        Args:
            X (torch.Tensor): Input tensor of shape (n_samples, n_features).

        Returns:
            torch.Tensor: Kernel matrix of shape (n_samples, n_samples).
        &#34;&#34;&#34;
        self.bandwidth = self.get_bandwidth(X)
        L2_distances = torch.cdist(X, X) ** 2
        return (1 + L2_distances / (2 * self.alpha)) ** (-self.alpha)</code></pre>
</details>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As per the example above, an <code>__init__()</code> call to the parent class
must be made before assignment on the child.</p>
</div>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Rational Quadratic Kernel.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>alpha</code></strong> :&ensp;<code>float</code></dt>
<dd>Shape parameter. Controls the tail behavior of the kernel.
Higher values make the kernel behave more like the RBF kernel.</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.models.Mmd_loss_constrained.RationalQuadratic.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, X) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, X):
    &#34;&#34;&#34;
    Compute the Rational Quadratic Kernel matrix.

    Args:
        X (torch.Tensor): Input tensor of shape (n_samples, n_features).

    Returns:
        torch.Tensor: Kernel matrix of shape (n_samples, n_samples).
    &#34;&#34;&#34;
    self.bandwidth = self.get_bandwidth(X)
    L2_distances = torch.cdist(X, X) ** 2
    return (1 + L2_distances / (2 * self.alpha)) ** (-self.alpha)</code></pre>
</details>
<div class="desc"><p>Compute the Rational Quadratic Kernel matrix.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Input tensor of shape (n_samples, n_features).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>Kernel matrix of shape (n_samples, n_samples).</dd>
</dl></div>
</dd>
<dt id="src.models.Mmd_loss_constrained.RationalQuadratic.get_bandwidth"><code class="name flex">
<span>def <span class="ident">get_bandwidth</span></span>(<span>self, L2_distances)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_bandwidth(self, L2_distances):
    n_samples = L2_distances.shape[0]
    self.bandwidth = L2_distances.data.sum() / (n_samples ** 2 - n_samples)
    return L2_distances.data.sum() / (n_samples ** 2 - n_samples)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.models" href="index.html">src.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.models.Mmd_loss_constrained.EfficientRBF" href="#src.models.Mmd_loss_constrained.EfficientRBF">EfficientRBF</a></code></h4>
<ul class="">
<li><code><a title="src.models.Mmd_loss_constrained.EfficientRBF.forward" href="#src.models.Mmd_loss_constrained.EfficientRBF.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.models.Mmd_loss_constrained.MMDLossConstrained" href="#src.models.Mmd_loss_constrained.MMDLossConstrained">MMDLossConstrained</a></code></h4>
<ul class="">
<li><code><a title="src.models.Mmd_loss_constrained.MMDLossConstrained.diversity_loss" href="#src.models.Mmd_loss_constrained.MMDLossConstrained.diversity_loss">diversity_loss</a></code></li>
<li><code><a title="src.models.Mmd_loss_constrained.MMDLossConstrained.forward" href="#src.models.Mmd_loss_constrained.MMDLossConstrained.forward">forward</a></code></li>
<li><code><a title="src.models.Mmd_loss_constrained.MMDLossConstrained.get_loss" href="#src.models.Mmd_loss_constrained.MMDLossConstrained.get_loss">get_loss</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.models.Mmd_loss_constrained.MixtureRQLinear" href="#src.models.Mmd_loss_constrained.MixtureRQLinear">MixtureRQLinear</a></code></h4>
<ul class="">
<li><code><a title="src.models.Mmd_loss_constrained.MixtureRQLinear.forward" href="#src.models.Mmd_loss_constrained.MixtureRQLinear.forward">forward</a></code></li>
<li><code><a title="src.models.Mmd_loss_constrained.MixtureRQLinear.get_bandwidth" href="#src.models.Mmd_loss_constrained.MixtureRQLinear.get_bandwidth">get_bandwidth</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.models.Mmd_loss_constrained.RBF" href="#src.models.Mmd_loss_constrained.RBF">RBF</a></code></h4>
<ul class="">
<li><code><a title="src.models.Mmd_loss_constrained.RBF.forward" href="#src.models.Mmd_loss_constrained.RBF.forward">forward</a></code></li>
<li><code><a title="src.models.Mmd_loss_constrained.RBF.get_bandwidth" href="#src.models.Mmd_loss_constrained.RBF.get_bandwidth">get_bandwidth</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.models.Mmd_loss_constrained.RationalQuadratic" href="#src.models.Mmd_loss_constrained.RationalQuadratic">RationalQuadratic</a></code></h4>
<ul class="">
<li><code><a title="src.models.Mmd_loss_constrained.RationalQuadratic.forward" href="#src.models.Mmd_loss_constrained.RationalQuadratic.forward">forward</a></code></li>
<li><code><a title="src.models.Mmd_loss_constrained.RationalQuadratic.get_bandwidth" href="#src.models.Mmd_loss_constrained.RationalQuadratic.get_bandwidth">get_bandwidth</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
