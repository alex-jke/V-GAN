<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>src.text.Embedding.LLM.llama API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.text.Embedding.LLM.llama</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.text.Embedding.LLM.llama.LLama"><code class="flex name class">
<span>class <span class="ident">LLama</span></span>
<span>(</span><span>max_token_length: int = 5120, debug: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LLama(HuggingModel, ABC):
    &#34;&#34;&#34;
    This class is a wrapper around the Llama model from Hugging Face&#39;s transformers library.
    &#34;&#34;&#34;
    @property
    def _model_name(self):
        return self.get_model_name()

    @property
    def _tokenizer(self):
        print(&#34;Tokenizer loaded&#34;)
        try:
            tokenizer = AutoTokenizer.from_pretrained(self._model_prefix + self.get_model_name(), trust_remote_code=True)
        except OSError as ose:
            print(&#34;An OSError was raised. This likely happened due to LLama being a restricted model. Please verify, that&#34;
                  &#34;you are logged into huggingface and have access to the LLama models used here. Check readme to see how to log in. The exception:&#34;)
            raise ose
        return tokenizer

    @property
    def _model_prefix(self):
        return &#34;meta-llama/&#34;

    @property
    def _model(self):
        print(&#34;Model loaded&#34;)
        # TODO: switching the model to run on bfloat16 causes infinite gradient norms.
        model = AutoModel.from_pretrained(self._model_prefix + self.get_model_name(), trust_remote_code=True, torch_dtype=torch.float16,
                                          attn_implementation=&#34;eager&#34;,
                                          device_map=&#34;auto&#34;)
                                          #).to(self.device)
        return model

    def fully_embed_tokenized(self, tokenized: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:

        mask = mask.unsqueeze(0) if mask is not None else None
        if mask is not None:
            input_embeds = self.embed_tokenized(tokenized).unsqueeze(0)

            #Mask needs to contain either 1.0 or 0.0, if other values are present, they are rounded
            if torch.logical_and(mask != 0.0, mask != 1.0).any():
                rounded_mask = torch.round(mask) + (mask - mask.detach())
            else:
                rounded_mask = mask
            causal_mask = self._get_4d_causal_mask(rounded_mask)
            input_embeds = input_embeds.to(self.model.get_input_embeddings().weight.data.dtype)
            torch.backends.cuda.enable_mem_efficient_sdp(False)
            torch.backends.cuda.enable_flash_sdp(False)
            #outputs = self.model(inputs_embeds=input_embeds, attention_mask=mask, output_attentions=True)
            outputs = self.model(inputs_embeds=input_embeds, attention_mask=causal_mask, use_cache=False)

        else:
            with torch.no_grad():
                if tokenized.shape[0] &gt; self.max_token_length():
                    if not self._token_length_warning_given:
                        logging.warning(
                            f&#34;Input contains token sequences of length larger than the maximum length {self.max_token_length()}. &#34;
                            f&#34;Found: {tokenized.shape[0]}. This token tensor and further token tensors of length&#34;
                            f&#34;larger than {self.max_token_length()} will be trimmed. No further warnings will be given.&#34;)
                        self._token_length_warning_given = True
                    tokenized = tokenized[:self.max_token_length()]
                unsqueezed = tokenized.int().unsqueeze(0)
                outputs = self.model(input_ids=unsqueezed, use_cache=False)
                #outputs = self.model(inputs_embeds=input_embeds, use_cache=False)
            outputs = outputs # Otherwise the line below complains about usage before assignment.
        embeddings = outputs[0]
        de_batched = embeddings[0]
        #de_batched = torch.nn.functional.normalize(de_batched, p=2, dim=-1)
        return de_batched

    def decode2tokenized(self, embedding: List[np.ndarray]) -&gt; List[int]:
        raise NotImplementedError

    @abstractmethod
    def get_model_name(self)-&gt;str:
        pass</code></pre>
</details>
<div class="desc"><p>This class is a wrapper around the Llama model from Hugging Face's transformers library.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>text.Embedding.LLM.huggingmodel.HuggingModel</li>
<li>text.Embedding.tokenizer.Tokenizer</li>
<li>text.Embedding.embedding.Embedding</li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="src.text.Embedding.LLM.llama.LLama1B" href="#src.text.Embedding.LLM.llama.LLama1B">LLama1B</a></li>
<li><a title="src.text.Embedding.LLM.llama.LLama3B" href="#src.text.Embedding.LLM.llama.LLama3B">LLama3B</a></li>
<li><a title="src.text.Embedding.LLM.llama.LLama3BInstruct" href="#src.text.Embedding.LLM.llama.LLama3BInstruct">LLama3BInstruct</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.text.Embedding.LLM.llama.LLama.decode2tokenized"><code class="name flex">
<span>def <span class="ident">decode2tokenized</span></span>(<span>self, embedding: List[numpy.ndarray]) ‑> List[int]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def decode2tokenized(self, embedding: List[np.ndarray]) -&gt; List[int]:
    raise NotImplementedError</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="src.text.Embedding.LLM.llama.LLama.fully_embed_tokenized"><code class="name flex">
<span>def <span class="ident">fully_embed_tokenized</span></span>(<span>self, tokenized: torch.Tensor, mask: torch.Tensor | None = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fully_embed_tokenized(self, tokenized: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:

    mask = mask.unsqueeze(0) if mask is not None else None
    if mask is not None:
        input_embeds = self.embed_tokenized(tokenized).unsqueeze(0)

        #Mask needs to contain either 1.0 or 0.0, if other values are present, they are rounded
        if torch.logical_and(mask != 0.0, mask != 1.0).any():
            rounded_mask = torch.round(mask) + (mask - mask.detach())
        else:
            rounded_mask = mask
        causal_mask = self._get_4d_causal_mask(rounded_mask)
        input_embeds = input_embeds.to(self.model.get_input_embeddings().weight.data.dtype)
        torch.backends.cuda.enable_mem_efficient_sdp(False)
        torch.backends.cuda.enable_flash_sdp(False)
        #outputs = self.model(inputs_embeds=input_embeds, attention_mask=mask, output_attentions=True)
        outputs = self.model(inputs_embeds=input_embeds, attention_mask=causal_mask, use_cache=False)

    else:
        with torch.no_grad():
            if tokenized.shape[0] &gt; self.max_token_length():
                if not self._token_length_warning_given:
                    logging.warning(
                        f&#34;Input contains token sequences of length larger than the maximum length {self.max_token_length()}. &#34;
                        f&#34;Found: {tokenized.shape[0]}. This token tensor and further token tensors of length&#34;
                        f&#34;larger than {self.max_token_length()} will be trimmed. No further warnings will be given.&#34;)
                    self._token_length_warning_given = True
                tokenized = tokenized[:self.max_token_length()]
            unsqueezed = tokenized.int().unsqueeze(0)
            outputs = self.model(input_ids=unsqueezed, use_cache=False)
            #outputs = self.model(inputs_embeds=input_embeds, use_cache=False)
        outputs = outputs # Otherwise the line below complains about usage before assignment.
    embeddings = outputs[0]
    de_batched = embeddings[0]
    #de_batched = torch.nn.functional.normalize(de_batched, p=2, dim=-1)
    return de_batched</code></pre>
</details>
<div class="desc"><p>This method expects a one-dimensional tensor of token indices and returns the corresponding embeddings.
:param tokenized: 1D Tensor of token indices.
:param mask: A mask to use to mask out certain tokens. Important to note that the mask is applied after tokenization.
This means that the mask should be of the same length as the tokenized data.
As an example, if the word "example" is tokenized to "['ex', 'amp', 'le']", the mask should be of length 3.
:return: two-dimensional Tensor where each token index is an embedding. (embedding_size, num_tokens)</p></div>
</dd>
<dt id="src.text.Embedding.LLM.llama.LLama.get_model_name"><code class="name flex">
<span>def <span class="ident">get_model_name</span></span>(<span>self) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def get_model_name(self)-&gt;str:
    pass</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="src.text.Embedding.LLM.llama.LLama1B"><code class="flex name class">
<span>class <span class="ident">LLama1B</span></span>
<span>(</span><span>max_token_length: int = 5120, debug: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LLama1B(LLama):
    def get_model_name(self)-&gt;str:
        return &#34;Llama-3.2-1B&#34;</code></pre>
</details>
<div class="desc"><p>This class is a wrapper around the Llama model from Hugging Face's transformers library.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="src.text.Embedding.LLM.llama.LLama" href="#src.text.Embedding.LLM.llama.LLama">LLama</a></li>
<li>text.Embedding.LLM.huggingmodel.HuggingModel</li>
<li>text.Embedding.tokenizer.Tokenizer</li>
<li>text.Embedding.embedding.Embedding</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.text.Embedding.LLM.llama.LLama1B.get_model_name"><code class="name flex">
<span>def <span class="ident">get_model_name</span></span>(<span>self) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_model_name(self)-&gt;str:
    return &#34;Llama-3.2-1B&#34;</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="src.text.Embedding.LLM.llama.LLama" href="#src.text.Embedding.LLM.llama.LLama">LLama</a></b></code>:
<ul class="hlist">
<li><code><a title="src.text.Embedding.LLM.llama.LLama.fully_embed_tokenized" href="#src.text.Embedding.LLM.llama.LLama.fully_embed_tokenized">fully_embed_tokenized</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="src.text.Embedding.LLM.llama.LLama3B"><code class="flex name class">
<span>class <span class="ident">LLama3B</span></span>
<span>(</span><span>max_token_length: int = 5120, debug: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LLama3B(LLama):
    def get_model_name(self)-&gt;str:
        return &#34;Llama-3.2-3B&#34;</code></pre>
</details>
<div class="desc"><p>This class is a wrapper around the Llama model from Hugging Face's transformers library.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="src.text.Embedding.LLM.llama.LLama" href="#src.text.Embedding.LLM.llama.LLama">LLama</a></li>
<li>text.Embedding.LLM.huggingmodel.HuggingModel</li>
<li>text.Embedding.tokenizer.Tokenizer</li>
<li>text.Embedding.embedding.Embedding</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.text.Embedding.LLM.llama.LLama3B.get_model_name"><code class="name flex">
<span>def <span class="ident">get_model_name</span></span>(<span>self) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_model_name(self)-&gt;str:
    return &#34;Llama-3.2-3B&#34;</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="src.text.Embedding.LLM.llama.LLama" href="#src.text.Embedding.LLM.llama.LLama">LLama</a></b></code>:
<ul class="hlist">
<li><code><a title="src.text.Embedding.LLM.llama.LLama.fully_embed_tokenized" href="#src.text.Embedding.LLM.llama.LLama.fully_embed_tokenized">fully_embed_tokenized</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="src.text.Embedding.LLM.llama.LLama3BInstruct"><code class="flex name class">
<span>class <span class="ident">LLama3BInstruct</span></span>
<span>(</span><span>max_token_length: int = 5120, debug: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LLama3BInstruct(LLama):
    def get_model_name(self) -&gt;str:
        return &#34;Llama-3.2-3B-Instruct&#34;</code></pre>
</details>
<div class="desc"><p>This class is a wrapper around the Llama model from Hugging Face's transformers library.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="src.text.Embedding.LLM.llama.LLama" href="#src.text.Embedding.LLM.llama.LLama">LLama</a></li>
<li>text.Embedding.LLM.huggingmodel.HuggingModel</li>
<li>text.Embedding.tokenizer.Tokenizer</li>
<li>text.Embedding.embedding.Embedding</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="src.text.Embedding.LLM.llama.LLama3BInstruct.get_model_name"><code class="name flex">
<span>def <span class="ident">get_model_name</span></span>(<span>self) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_model_name(self) -&gt;str:
    return &#34;Llama-3.2-3B-Instruct&#34;</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="src.text.Embedding.LLM.llama.LLama" href="#src.text.Embedding.LLM.llama.LLama">LLama</a></b></code>:
<ul class="hlist">
<li><code><a title="src.text.Embedding.LLM.llama.LLama.fully_embed_tokenized" href="#src.text.Embedding.LLM.llama.LLama.fully_embed_tokenized">fully_embed_tokenized</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.text.Embedding.LLM" href="index.html">src.text.Embedding.LLM</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.text.Embedding.LLM.llama.LLama" href="#src.text.Embedding.LLM.llama.LLama">LLama</a></code></h4>
<ul class="">
<li><code><a title="src.text.Embedding.LLM.llama.LLama.decode2tokenized" href="#src.text.Embedding.LLM.llama.LLama.decode2tokenized">decode2tokenized</a></code></li>
<li><code><a title="src.text.Embedding.LLM.llama.LLama.fully_embed_tokenized" href="#src.text.Embedding.LLM.llama.LLama.fully_embed_tokenized">fully_embed_tokenized</a></code></li>
<li><code><a title="src.text.Embedding.LLM.llama.LLama.get_model_name" href="#src.text.Embedding.LLM.llama.LLama.get_model_name">get_model_name</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.text.Embedding.LLM.llama.LLama1B" href="#src.text.Embedding.LLM.llama.LLama1B">LLama1B</a></code></h4>
<ul class="">
<li><code><a title="src.text.Embedding.LLM.llama.LLama1B.get_model_name" href="#src.text.Embedding.LLM.llama.LLama1B.get_model_name">get_model_name</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.text.Embedding.LLM.llama.LLama3B" href="#src.text.Embedding.LLM.llama.LLama3B">LLama3B</a></code></h4>
<ul class="">
<li><code><a title="src.text.Embedding.LLM.llama.LLama3B.get_model_name" href="#src.text.Embedding.LLM.llama.LLama3B.get_model_name">get_model_name</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="src.text.Embedding.LLM.llama.LLama3BInstruct" href="#src.text.Embedding.LLM.llama.LLama3BInstruct">LLama3BInstruct</a></code></h4>
<ul class="">
<li><code><a title="src.text.Embedding.LLM.llama.LLama3BInstruct.get_model_name" href="#src.text.Embedding.LLM.llama.LLama3BInstruct.get_model_name">get_model_name</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
