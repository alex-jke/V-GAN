<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>src.text.Embedding.LLM.huggingmodel API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.text.Embedding.LLM.huggingmodel</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.text.Embedding.LLM.huggingmodel.HuggingModel"><code class="flex name class">
<span>class <span class="ident">HuggingModel</span></span>
<span>(</span><span>max_token_length: int = 5120, debug: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HuggingModel(Tokenizer, Embedding, ABC):

    @property
    @abstractmethod
    def _model_name(self):
        pass


    @property
    @abstractmethod
    def _tokenizer(self):
        pass

    @property
    @abstractmethod
    def _model(self):
        pass


    def __init__(self, max_token_length: int = 5120, debug: bool = True):
        super().__init__()
        self.device = torch.device(&#39;cuda&#39; if torch.cuda.is_available(
        ) else &#39;mps&#39; if torch.backends.mps.is_available() else &#39;cpu&#39;)
        #print(f&#34;Using device: {self.device}, cuda: {torch.cuda.is_available()}, mps: {torch.backends.mps.is_available()}&#34;)
        self.__tokenizer = None
        self.__model: Optional[HuggingModel] = None
        #self.model = self._model
        #if torch.cuda.device_count() &gt; 1:
            #print(f&#34;Using {torch.cuda.device_count()} GPUs.&#34;)
            #model = nn.DataParallel(model)
        #self.model = model.to(self.device)
        self.model_name = self._model_name
        self._max_token_length = max_token_length
        self.__padding_token = None
        #self.padding_token = self._padding_token
        self.ui = cli.get()
        self.prefix_mask: Optional[Tensor] = None
        self.suffix_mask: Optional[Tensor] = None
        self._token_length_warning_given = False
        self.debug = debug

    @property
    def padding_token(self):
        if self.__padding_token is None:
            self.__padding_token = self._padding_token
        return self.__padding_token


    @property
    def model(self):
        if self.__model is None:
            self.__model = self._model
            #self.__model.eval()
            self.__model.train()
            self.model.gradient_checkpointing_enable()
            for param in self.__model.parameters():
                param.requires_grad = False
        return self.__model

    @property
    def tokenizer(self):
        if self.__tokenizer is None:
            self.__tokenizer = self._tokenizer
        return self.__tokenizer

    def tokenize(self, data: str) -&gt; Tensor: #List[int]:
        tokenized = self.tokenizer(data, return_tensors=&#39;pt&#39;)
        input_ids = tokenized[&#39;input_ids&#39;]
        #input_list = input_ids.tolist()
        #first_elem = input_list[0]
        #return first_elem
        first_elem = input_ids[0]
        return first_elem

    def tokenize_batch(self, data: List[str]) -&gt; Tensor:
        if len(data) == 0:
            raise ValueError(&#34;No data to tokenize.&#34;)
        with self.ui.display():
            tokenized_list = []
            for i, d in enumerate(data):
                self.ui.update(f&#34;Tokenizing {i+1}/{len(data)}&#34;)
                tokenized_list.append(Tensor(self.tokenize(d)))
            #tokenized_list = [Tensor(self.tokenize(d)) for d in data]
        max_length = max([t.shape[0] for t in tokenized_list])
        padded_token_list = [torch.nn.functional.pad(t, (0, max_length - len(t)), value=self.padding_token) for t in tokenized_list]
        tensor = torch.stack(padded_token_list).int().to(self.device)
        return tensor

    def detokenize(self, words: List[int]) -&gt; str:
        return self.tokenizer.decode(words)

    def embed(self, data: str, mask: Optional[Tensor] = None) -&gt; Tensor:
        &#34;&#34;&#34;
        Embeds a single string into a vector. The string is tokenized and then embedded.
        An optional mask can be used to mask out certain tokens.
        :param data: The string to embed.
        :param mask: A mask to use to mask out certain tokens. Important to note that the mask is applied after tokenization.
            This means that the mask should be of the same length as the tokenized data.
            As an example, if the word &#34;example&#34; is tokenized to &#34;[&#39;ex&#39;, &#39;amp&#39;, &#39;le&#39;]&#34;, the mask should be of length 3.
        :return: The embedding of the string, as a tensor of shape (token_amount, embedding_dim).
        &#34;&#34;&#34;
        tokenized = self.tokenize(data).to(self.device)
        return self.fully_embed_tokenized(tokenized, mask)

    def _convert_word_to_token_mask(self, tokenized: List[Tensor], mask: Tensor) -&gt; Tensor:
        token_word_lengths = Tensor([tokens.shape[0] for tokens in tokenized]).int().to(self.device)
        assert token_word_lengths.shape[0] == mask.shape[0], f&#34;The word mask length and word lengths do not match. Mask: {mask.shape[0]}, Tokens: {token_word_lengths.shape[0]} &#34;
        try:
            mask = mask.repeat_interleave(token_word_lengths, dim=0)
        except RuntimeError as e:
            raise e
        return mask

    def _aggregate_token_to_word_embedding(self, embedded: Tensor, tokenized: List[Tensor]) -&gt; Tensor:
        &#34;&#34;&#34;
        Aggregates the token embeddings to word embeddings. The aggregation is done by averaging the token embeddings.
        :param embedded: The embeddings of the tokens.
        :param tokenized: The tokenized data.
        :return: The embeddings of the words.
            As an example, if the word &#34;example&#34; is tokenized to &#34;[&#39;ex&#39;, &#39;amp&#39;, &#39;le&#39;]&#34; and the embeddings of the tokens are
            {&#39;ex&#39;: [1, 2], &#39;amp&#39;: [3, 4], &#39;le&#39;: [5, 6]}, the word embedding would be [3, 4].
        &#34;&#34;&#34;
        word_embeddings = Tensor().to(embedded.device)
        start_index = 0
        for i, tokens in enumerate(tokenized):
            n_embeddings = tokens.shape[0]
            end_index = start_index + n_embeddings
            assert end_index &lt;= embedded.shape[0]
            if n_embeddings == 1:
                word_embedding = embedded[start_index].unsqueeze(0)
            else:
                word_embedding = embedded[start_index:end_index].mean(dim=0).unsqueeze(0)
                #word_embedding = embedded[end_index -1].unsqueeze(0)
            start_index = end_index
            word_embeddings = torch.concat((word_embeddings, word_embedding), dim=0)
        return word_embeddings


    def _tokenize_words(self, words: List[str], max_token_length: Optional[int] = None) -&gt; List[Tensor]:
        tokenized = []
        #previous_words = []
        previous_word: Optional[str] = None
        previous_length = 0
        prefix = &#34;I&#34; # Add a prefix to the tokenization of the words, as sometimes the token changes, if at the front of a sequence.
        prefix_tokens = self.tokenize(prefix)
        prefix_length = prefix_tokens.shape[0]
        current_length = 0
        for word in words:
            # Only the first word keeps the begin of input token.

            #previous_words.append(word)
            current_sequence = &#34; &#34;.join([prefix, word]) if previous_word is not None else word
            tokens = self.tokenize(current_sequence)
            if len(tokenized) != 0:
                tokens = tokens[prefix_length:]

            if max_token_length is not None and tokens.shape[0] + current_length &gt; max_token_length:
                logging.warning(f&#34;Tried tokenizing sequence longer than maximum sequence length of &#34;
                                f&#34;{self.max_token_length()}. The input will be trimmed.&#34;)
                break
            tokenized.append(tokens)
            previous_word = word
            current_length += tokens.shape[0]
        if self.debug:
            concat = torch.concat(tokenized, dim=0)
            assert concat.equal(self.tokenize(&#34; &#34;.join(words))[:concat.shape[0]])
        return tokenized


    def _embed_words_full(self, words: List[str], mask: Optional[Tensor] = None, suffix: Optional[List[str]] = None) -&gt; Tensor:
        if mask is not None and mask.shape[0] &gt; self.max_word_length():
            raise RuntimeError(f&#34;A mask was passed, thats length is longer that the maximum amount: {mask.shape[0] &gt; self.max_word_length()}&#34;)

        if mask is not None:
            assert len(words) + len(suffix) == mask.shape[0], f&#34;The provided word mask and amount of words do not match up, mask: {mask.shape[0]}, words: {len(words)}\n{mask}\n{words}&#34;

        if suffix is not None:
            tokenized_suffix = self._tokenize_words(words=suffix)
            tokenized_suffix[0] = tokenized_suffix[0][1:] # Remove the beginning of sequence token.
            suffix_length = sum([tokens.shape[0] for tokens in tokenized_suffix])
            tokenized_sample = self._tokenize_words(words=words, max_token_length= self.max_token_length() - suffix_length)
            tokenized = tokenized_sample + tokenized_suffix
            if mask is not None:
                assert len(tokenized) == mask.shape[0], f&#34;The amount of tokenized words and word mask do not match up, tokenized: {len(tokenized)}, mask: {mask.shape[0]}&#34;
        else:
            tokenized = self._tokenize_words(words=words, max_token_length=self.max_token_length())
            if mask is not None:
                assert len(tokenized) == mask.shape[0], f&#34;The amount of tokenized words and word mask do not match up, tokenized: {len(tokenized)}, mask: {mask.shape[0]}&#34;
        expanded_mask = self._convert_word_to_token_mask(tokenized, mask) if mask is not None else None
        if expanded_mask is not None and expanded_mask.shape[0] &gt; self.max_token_length():
            raise ValueError(f&#34;Passed a mask that is longer, than the maximum token length. {self.max_token_length()}.&#34;)
        #embeddings = self.embed(sentence, expanded_mask)
        tokenized_tensor = torch.concat(tokenized, dim=0).to(self.device).float()
        if tokenized_tensor.shape[0] &gt; self.max_token_length():
            raise RuntimeError(&#34;Tokenized tensor is longer than max token length. This should not occur and signifies,&#34;
                               &#34;that something is wrong with the truncation process before.&#34;)
        embeddings = self.fully_embed_tokenized(tokenized_tensor, expanded_mask)
        aggregated = self._aggregate_token_to_word_embedding(embeddings, tokenized)

        #aggregated = torch.nn.functional.normalize(aggregated, dim=1)

        # Apply the mask to the embeddings, so that the masked tokens are zeroed out
        masked = aggregated * mask.to(aggregated.device).unsqueeze(1).expand_as(aggregated) if mask is not None else aggregated
        return masked

    def get_prefix_mask(self) -&gt; Tensor:
        return torch.ones(len(self.prefix)).to(self.device)

    def get_suffix_mask(self) -&gt; Tensor:
        return torch.ones(len(self.suffix)).to(self.device)

    def _embed_words_last(self, words: List[str], mask: Optional[Tensor] = None) -&gt; Tensor:

        classification_added_words = self.prefix + words + self.suffix
        #added_mask = Tensor([1, 1, 1]).to(self.device) if mask is not None else None
        prefix_mask = self.get_prefix_mask()
        suffix_mask = self.get_suffix_mask()
        classification_added_mask = torch.concat((prefix_mask, mask, suffix_mask)) if mask is not None else None
        if mask is not None:
            assert prefix_mask.shape[0] == len(self.prefix), f&#34;Length of prefix and prefix mask do not agree: mask: {prefix_mask.shape[0]}, prefix words: {len(self.prefix)}&#34;
            assert suffix_mask.shape[0] == len(self.suffix), f&#34;Length of suffix and suffix mask do not agree: mask: {suffix_mask.shape[0]}, suffix words: {len(self.prefix)}&#34;
            assert mask.shape[0] == len(words), f&#34;The amount of words and length of mask do not align: mask: {mask.shape[0]}, words: {len(words)}&#34;
        masked = self._embed_words_full(self.prefix + words, classification_added_mask, suffix=self.suffix)
        last_entry = masked[-1]
        expanded = last_entry.unsqueeze(0)
        return expanded

    def embed_words(self, words: List[str], mask: Optional[Tensor] = None, strategy: StrategyInstance = UnificationStrategy.TRANSFORMER.create(), trim: bool = True) -&gt; Tensor:
        if trim:
            words = words[:self.max_word_length()]
        if strategy.equals(UnificationStrategy.TRANSFORMER):
            return torch.nn.functional.normalize(self._embed_words_last(words, mask), dim=1)
        if strategy.equals(UnificationStrategy.MEAN):
            return torch.nn.functional.normalize(self._embed_words_full(words, mask).mean(dim=0).unsqueeze(0), dim=1)
        if strategy.equals(UnificationStrategy.PADDING):
            return self._embed_words_full(words, mask)
        raise NotImplementedError(f&#34;The strategy {strategy} is not implemented for huggingmodels.&#34;)

    def embed_tokenized(self, tokenized: Tensor) -&gt; Tensor:
        &#34;&#34;&#34;
        This method takes a list of token indices and returns the
         corresponding embeddings for the first layer of the transformer.
         That is without any context.
        :param tokenized: A tensor of token indices of shape (num_tokens).
        :return: A two-dimensional Tensor where each token index is an embedding. (num_tokens, embedding_size)
        &#34;&#34;&#34;
        max_length = self.tokenizer.model_max_length
        token_vec = tokenized[:max_length]
        input_embeds_mat = self.model.get_input_embeddings().weight.data
        one_hot = (F.one_hot(token_vec.long(), input_embeds_mat.shape[0]).float() + (
                    token_vec - token_vec.detach()).unsqueeze(1)).to(input_embeds_mat.dtype).to(input_embeds_mat.device)
        inputs_embeds = one_hot @ input_embeds_mat
        return inputs_embeds

    @abstractmethod
    def fully_embed_tokenized(self, tokenized: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:
        &#34;&#34;&#34;
        This method expects a one-dimensional tensor of token indices and returns the corresponding embeddings.
        :param tokenized: 1D Tensor of token indices.
        :param mask: A mask to use to mask out certain tokens. Important to note that the mask is applied after tokenization.
            This means that the mask should be of the same length as the tokenized data.
            As an example, if the word &#34;example&#34; is tokenized to &#34;[&#39;ex&#39;, &#39;amp&#39;, &#39;le&#39;]&#34;, the mask should be of length 3.
        :return: two-dimensional Tensor where each token index is an embedding. (embedding_size, num_tokens)
        &#34;&#34;&#34;
        pass


    @property
    def _padding_token(self) -&gt; int:
        token = self.tokenizer.pad_token_id
        if token is None:
            token = self.tokenizer.eos_token_id
        if token is None:
            raise ValueError(&#34;No padding token found in tokenizer.&#34;)
        #return token
        return 0 #todo: might cause conflict with another symbol

    def aggregateEmbeddings(self, embeddings: Tensor):
        &#34;&#34;&#34;
        The function defines how given a collection of embeddings it is aggregated into a single embedding.
        :param embeddings: A three-dimensional tensor of embeddings. The first dimension is the size of the embeddings,
        the second is the batch dimension and the third is the number of tokens
        :return: A two-dimensional Tensor, where the first dimension is the batch dimension and the second is the
        embedding dimension
        &#34;&#34;&#34;
        aggregated = embeddings.mean(dim=-1)
        return aggregated

    def get_embedding_fun(self, batch_first = False, remove_padding = False) -&gt; Callable[[Tensor], Tensor]:
        def embedding(data: Tensor) -&gt; Tensor:
            &#34;&#34;&#34;
            This method takes a tensor of tokenized datapoints and returns the embeddings.
            :param data: A two-dimensional tensor of tokenized datapoints. The first dimension is the number of datapoints and the
            second is the number of tokens.
            :return: A two-dimensional Tensor aggregated in accordance to the aggregate function.
            &#34;&#34;&#34;
            embeddings = torch.tensor([], dtype=torch.int).to(self.device)
            ui = cli.get()
            #with torch.no_grad():#, ui.display():
            longest_sequence = data.shape[1]
            #print(f&#34;Longest sequence is {longest_sequence} tokens.&#34;)
            with ui.display():

                for (i, partial_review) in enumerate(data):
                    ui.update(f&#34;Embedding {i+1}/{len(data)}&#34;)
                    partial_review: Tensor

                    if remove_padding:
                        # Remove padding tokens
                        partial_review = partial_review[partial_review != self.padding_token]

                    embedded: Tensor = self.fully_embed_tokenized(partial_review).T #returns a (embedding_size, num_tokens) tensor
                     #add extra third dimension
                    unsqueezed = embedded.unsqueeze(1)
                    aggregated = self.aggregateEmbeddings(embeddings = unsqueezed)

                    try:
                        embeddings = torch.cat((embeddings, aggregated), dim=1)
                    except:
                        print(embeddings.shape, aggregated.shape)
                        raise
                #aggregated = self.aggregateEmbeddings(embeddings = embeddings)
            if batch_first:
                transposed = embeddings.T
                #meaned = transposed - transposed.mean(dim=0)
                normed = torch.nn.functional.normalize(transposed, dim=1)

                return normed
                #return transposed
            return embeddings
        return embedding

    def max_token_length(self) -&gt; int:
        return min(self.tokenizer.model_max_length, self._max_token_length)

    def max_word_length(self) -&gt; int:
        return int(self.max_token_length() / WORDS_TO_TOKENS_FACTOR)

    def _get_4d_causal_mask(self, attention_mask: Tensor) -&gt; Tensor:
        &#34;&#34;&#34;
            Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape
            `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.

            Args:
                attention_mask (`torch.Tensor`):
                    A 2D attention mask of shape `(batch_size, key_value_length)`
            [Taken from the model&#39;s source code. and adjusted to be able to pass the gradient]
        &#34;&#34;&#34;
        sequence_length = target_length = attention_mask.shape[-1]
        batch_size = attention_mask.shape[0]
        dtype = self.model.get_input_embeddings().weight.data.dtype
        device = self.device
        cache_position = torch.arange(sequence_length, device=device)
        causal_mask = torch.zeros((sequence_length, target_length), dtype=dtype, device=device)
        if sequence_length != 1:
            # Create an upper triangular mask (1 for positions to mask, 0 for unmasked)
            causal_mask = torch.triu(torch.ones_like(causal_mask), diagonal=1)
        # Apply the cache condition (1 for masked positions, 0 for allowed positions)
        cache_cond = (torch.arange(target_length, device=device, dtype=dtype) &gt; cache_position.reshape(-1, 1))
        causal_mask = causal_mask * cache_cond.to(dtype)
        # Convert binary mask to additive mask: 1 becomes min_dtype and 0 stays 0
        min_dtype = torch.finfo(dtype).min
        causal_mask = causal_mask * (min_dtype - 0)
        # Expand to 4D
        causal_mask = causal_mask.unsqueeze(0).unsqueeze(0).expand(batch_size, 1, -1, -1)
        if attention_mask is not None:
            mask_length = attention_mask.shape[-1]
            causal_slice = causal_mask[:, :, :, :mask_length]
            # Use a differentiable combination:
            # When attention_mask is 1, use the causal value; when 0, use min_dtype.
            combined = causal_slice * attention_mask[:, None, None, :] + min_dtype * (
                        1 - attention_mask[:, None, None, :])
            # Replace the slice with the combined result
            causal_mask = torch.cat([combined, causal_mask[:, :, :, mask_length:]], dim=-1)

        return causal_mask.to(dtype)</code></pre>
</details>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>text.Embedding.tokenizer.Tokenizer</li>
<li>text.Embedding.embedding.Embedding</li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="src.text.Embedding.LLM.bert.Bert" href="bert.html#src.text.Embedding.LLM.bert.Bert">Bert</a></li>
<li><a title="src.text.Embedding.LLM.gpt2.GPT2" href="gpt2.html#src.text.Embedding.LLM.gpt2.GPT2">GPT2</a></li>
<li>text.Embedding.LLM.gpt2.GPT2</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="src.text.Embedding.LLM.huggingmodel.HuggingModel.model"><code class="name">prop <span class="ident">model</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def model(self):
    if self.__model is None:
        self.__model = self._model
        #self.__model.eval()
        self.__model.train()
        self.model.gradient_checkpointing_enable()
        for param in self.__model.parameters():
            param.requires_grad = False
    return self.__model</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="src.text.Embedding.LLM.huggingmodel.HuggingModel.padding_token"><code class="name">prop <span class="ident">padding_token</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def padding_token(self):
    if self.__padding_token is None:
        self.__padding_token = self._padding_token
    return self.__padding_token</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="src.text.Embedding.LLM.huggingmodel.HuggingModel.tokenizer"><code class="name">prop <span class="ident">tokenizer</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def tokenizer(self):
    if self.__tokenizer is None:
        self.__tokenizer = self._tokenizer
    return self.__tokenizer</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="src.text.Embedding.LLM.huggingmodel.HuggingModel.aggregateEmbeddings"><code class="name flex">
<span>def <span class="ident">aggregateEmbeddings</span></span>(<span>self, embeddings: torch.Tensor)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def aggregateEmbeddings(self, embeddings: Tensor):
    &#34;&#34;&#34;
    The function defines how given a collection of embeddings it is aggregated into a single embedding.
    :param embeddings: A three-dimensional tensor of embeddings. The first dimension is the size of the embeddings,
    the second is the batch dimension and the third is the number of tokens
    :return: A two-dimensional Tensor, where the first dimension is the batch dimension and the second is the
    embedding dimension
    &#34;&#34;&#34;
    aggregated = embeddings.mean(dim=-1)
    return aggregated</code></pre>
</details>
<div class="desc"><p>The function defines how given a collection of embeddings it is aggregated into a single embedding.
:param embeddings: A three-dimensional tensor of embeddings. The first dimension is the size of the embeddings,
the second is the batch dimension and the third is the number of tokens
:return: A two-dimensional Tensor, where the first dimension is the batch dimension and the second is the
embedding dimension</p></div>
</dd>
<dt id="src.text.Embedding.LLM.huggingmodel.HuggingModel.detokenize"><code class="name flex">
<span>def <span class="ident">detokenize</span></span>(<span>self, words: List[int]) ‑> str</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def detokenize(self, words: List[int]) -&gt; str:
    return self.tokenizer.decode(words)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="src.text.Embedding.LLM.huggingmodel.HuggingModel.embed"><code class="name flex">
<span>def <span class="ident">embed</span></span>(<span>self, data: str, mask: torch.Tensor | None = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def embed(self, data: str, mask: Optional[Tensor] = None) -&gt; Tensor:
    &#34;&#34;&#34;
    Embeds a single string into a vector. The string is tokenized and then embedded.
    An optional mask can be used to mask out certain tokens.
    :param data: The string to embed.
    :param mask: A mask to use to mask out certain tokens. Important to note that the mask is applied after tokenization.
        This means that the mask should be of the same length as the tokenized data.
        As an example, if the word &#34;example&#34; is tokenized to &#34;[&#39;ex&#39;, &#39;amp&#39;, &#39;le&#39;]&#34;, the mask should be of length 3.
    :return: The embedding of the string, as a tensor of shape (token_amount, embedding_dim).
    &#34;&#34;&#34;
    tokenized = self.tokenize(data).to(self.device)
    return self.fully_embed_tokenized(tokenized, mask)</code></pre>
</details>
<div class="desc"><p>Embeds a single string into a vector. The string is tokenized and then embedded.
An optional mask can be used to mask out certain tokens.
:param data: The string to embed.
:param mask: A mask to use to mask out certain tokens. Important to note that the mask is applied after tokenization.
This means that the mask should be of the same length as the tokenized data.
As an example, if the word "example" is tokenized to "['ex', 'amp', 'le']", the mask should be of length 3.
:return: The embedding of the string, as a tensor of shape (token_amount, embedding_dim).</p></div>
</dd>
<dt id="src.text.Embedding.LLM.huggingmodel.HuggingModel.embed_tokenized"><code class="name flex">
<span>def <span class="ident">embed_tokenized</span></span>(<span>self, tokenized: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def embed_tokenized(self, tokenized: Tensor) -&gt; Tensor:
    &#34;&#34;&#34;
    This method takes a list of token indices and returns the
     corresponding embeddings for the first layer of the transformer.
     That is without any context.
    :param tokenized: A tensor of token indices of shape (num_tokens).
    :return: A two-dimensional Tensor where each token index is an embedding. (num_tokens, embedding_size)
    &#34;&#34;&#34;
    max_length = self.tokenizer.model_max_length
    token_vec = tokenized[:max_length]
    input_embeds_mat = self.model.get_input_embeddings().weight.data
    one_hot = (F.one_hot(token_vec.long(), input_embeds_mat.shape[0]).float() + (
                token_vec - token_vec.detach()).unsqueeze(1)).to(input_embeds_mat.dtype).to(input_embeds_mat.device)
    inputs_embeds = one_hot @ input_embeds_mat
    return inputs_embeds</code></pre>
</details>
<div class="desc"><p>This method takes a list of token indices and returns the
corresponding embeddings for the first layer of the transformer.
That is without any context.
:param tokenized: A tensor of token indices of shape (num_tokens).
:return: A two-dimensional Tensor where each token index is an embedding. (num_tokens, embedding_size)</p></div>
</dd>
<dt id="src.text.Embedding.LLM.huggingmodel.HuggingModel.embed_words"><code class="name flex">
<span>def <span class="ident">embed_words</span></span>(<span>self,<br>words: List[str],<br>mask: torch.Tensor | None = None,<br>strategy: text.Embedding.unification_strategy.StrategyInstance = TRANSFORMER,<br>trim: bool = True) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def embed_words(self, words: List[str], mask: Optional[Tensor] = None, strategy: StrategyInstance = UnificationStrategy.TRANSFORMER.create(), trim: bool = True) -&gt; Tensor:
    if trim:
        words = words[:self.max_word_length()]
    if strategy.equals(UnificationStrategy.TRANSFORMER):
        return torch.nn.functional.normalize(self._embed_words_last(words, mask), dim=1)
    if strategy.equals(UnificationStrategy.MEAN):
        return torch.nn.functional.normalize(self._embed_words_full(words, mask).mean(dim=0).unsqueeze(0), dim=1)
    if strategy.equals(UnificationStrategy.PADDING):
        return self._embed_words_full(words, mask)
    raise NotImplementedError(f&#34;The strategy {strategy} is not implemented for huggingmodels.&#34;)</code></pre>
</details>
<div class="desc"><p>Embeds a list of words into vectors.
:param words: The list of words to embed.
:param mask: The mask to use. If None, all words are embedded.
:param strategy: The unification strategy to use.
:param trim: Whether to trim the amount words list, if exceeding the maximum amount of words.
:return: The embeddings of the words, as a numpy array, representing a list of vectors.
The shape of the array should be (n_words, n_dim).</p></div>
</dd>
<dt id="src.text.Embedding.LLM.huggingmodel.HuggingModel.fully_embed_tokenized"><code class="name flex">
<span>def <span class="ident">fully_embed_tokenized</span></span>(<span>self, tokenized: torch.Tensor, mask: torch.Tensor | None = None) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def fully_embed_tokenized(self, tokenized: Tensor, mask: Optional[Tensor] = None) -&gt; Tensor:
    &#34;&#34;&#34;
    This method expects a one-dimensional tensor of token indices and returns the corresponding embeddings.
    :param tokenized: 1D Tensor of token indices.
    :param mask: A mask to use to mask out certain tokens. Important to note that the mask is applied after tokenization.
        This means that the mask should be of the same length as the tokenized data.
        As an example, if the word &#34;example&#34; is tokenized to &#34;[&#39;ex&#39;, &#39;amp&#39;, &#39;le&#39;]&#34;, the mask should be of length 3.
    :return: two-dimensional Tensor where each token index is an embedding. (embedding_size, num_tokens)
    &#34;&#34;&#34;
    pass</code></pre>
</details>
<div class="desc"><p>This method expects a one-dimensional tensor of token indices and returns the corresponding embeddings.
:param tokenized: 1D Tensor of token indices.
:param mask: A mask to use to mask out certain tokens. Important to note that the mask is applied after tokenization.
This means that the mask should be of the same length as the tokenized data.
As an example, if the word "example" is tokenized to "['ex', 'amp', 'le']", the mask should be of length 3.
:return: two-dimensional Tensor where each token index is an embedding. (embedding_size, num_tokens)</p></div>
</dd>
<dt id="src.text.Embedding.LLM.huggingmodel.HuggingModel.get_embedding_fun"><code class="name flex">
<span>def <span class="ident">get_embedding_fun</span></span>(<span>self, batch_first=False, remove_padding=False) ‑> Callable[[torch.Tensor], torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_embedding_fun(self, batch_first = False, remove_padding = False) -&gt; Callable[[Tensor], Tensor]:
    def embedding(data: Tensor) -&gt; Tensor:
        &#34;&#34;&#34;
        This method takes a tensor of tokenized datapoints and returns the embeddings.
        :param data: A two-dimensional tensor of tokenized datapoints. The first dimension is the number of datapoints and the
        second is the number of tokens.
        :return: A two-dimensional Tensor aggregated in accordance to the aggregate function.
        &#34;&#34;&#34;
        embeddings = torch.tensor([], dtype=torch.int).to(self.device)
        ui = cli.get()
        #with torch.no_grad():#, ui.display():
        longest_sequence = data.shape[1]
        #print(f&#34;Longest sequence is {longest_sequence} tokens.&#34;)
        with ui.display():

            for (i, partial_review) in enumerate(data):
                ui.update(f&#34;Embedding {i+1}/{len(data)}&#34;)
                partial_review: Tensor

                if remove_padding:
                    # Remove padding tokens
                    partial_review = partial_review[partial_review != self.padding_token]

                embedded: Tensor = self.fully_embed_tokenized(partial_review).T #returns a (embedding_size, num_tokens) tensor
                 #add extra third dimension
                unsqueezed = embedded.unsqueeze(1)
                aggregated = self.aggregateEmbeddings(embeddings = unsqueezed)

                try:
                    embeddings = torch.cat((embeddings, aggregated), dim=1)
                except:
                    print(embeddings.shape, aggregated.shape)
                    raise
            #aggregated = self.aggregateEmbeddings(embeddings = embeddings)
        if batch_first:
            transposed = embeddings.T
            #meaned = transposed - transposed.mean(dim=0)
            normed = torch.nn.functional.normalize(transposed, dim=1)

            return normed
            #return transposed
        return embeddings
    return embedding</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="src.text.Embedding.LLM.huggingmodel.HuggingModel.get_prefix_mask"><code class="name flex">
<span>def <span class="ident">get_prefix_mask</span></span>(<span>self) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_prefix_mask(self) -&gt; Tensor:
    return torch.ones(len(self.prefix)).to(self.device)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="src.text.Embedding.LLM.huggingmodel.HuggingModel.get_suffix_mask"><code class="name flex">
<span>def <span class="ident">get_suffix_mask</span></span>(<span>self) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_suffix_mask(self) -&gt; Tensor:
    return torch.ones(len(self.suffix)).to(self.device)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="src.text.Embedding.LLM.huggingmodel.HuggingModel.max_token_length"><code class="name flex">
<span>def <span class="ident">max_token_length</span></span>(<span>self) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def max_token_length(self) -&gt; int:
    return min(self.tokenizer.model_max_length, self._max_token_length)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="src.text.Embedding.LLM.huggingmodel.HuggingModel.max_word_length"><code class="name flex">
<span>def <span class="ident">max_word_length</span></span>(<span>self) ‑> int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def max_word_length(self) -&gt; int:
    return int(self.max_token_length() / WORDS_TO_TOKENS_FACTOR)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="src.text.Embedding.LLM.huggingmodel.HuggingModel.tokenize"><code class="name flex">
<span>def <span class="ident">tokenize</span></span>(<span>self, data: str) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tokenize(self, data: str) -&gt; Tensor: #List[int]:
    tokenized = self.tokenizer(data, return_tensors=&#39;pt&#39;)
    input_ids = tokenized[&#39;input_ids&#39;]
    #input_list = input_ids.tolist()
    #first_elem = input_list[0]
    #return first_elem
    first_elem = input_ids[0]
    return first_elem</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="src.text.Embedding.LLM.huggingmodel.HuggingModel.tokenize_batch"><code class="name flex">
<span>def <span class="ident">tokenize_batch</span></span>(<span>self, data: List[str]) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def tokenize_batch(self, data: List[str]) -&gt; Tensor:
    if len(data) == 0:
        raise ValueError(&#34;No data to tokenize.&#34;)
    with self.ui.display():
        tokenized_list = []
        for i, d in enumerate(data):
            self.ui.update(f&#34;Tokenizing {i+1}/{len(data)}&#34;)
            tokenized_list.append(Tensor(self.tokenize(d)))
        #tokenized_list = [Tensor(self.tokenize(d)) for d in data]
    max_length = max([t.shape[0] for t in tokenized_list])
    padded_token_list = [torch.nn.functional.pad(t, (0, max_length - len(t)), value=self.padding_token) for t in tokenized_list]
    tensor = torch.stack(padded_token_list).int().to(self.device)
    return tensor</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.text.Embedding.LLM" href="index.html">src.text.Embedding.LLM</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.text.Embedding.LLM.huggingmodel.HuggingModel" href="#src.text.Embedding.LLM.huggingmodel.HuggingModel">HuggingModel</a></code></h4>
<ul class="">
<li><code><a title="src.text.Embedding.LLM.huggingmodel.HuggingModel.aggregateEmbeddings" href="#src.text.Embedding.LLM.huggingmodel.HuggingModel.aggregateEmbeddings">aggregateEmbeddings</a></code></li>
<li><code><a title="src.text.Embedding.LLM.huggingmodel.HuggingModel.detokenize" href="#src.text.Embedding.LLM.huggingmodel.HuggingModel.detokenize">detokenize</a></code></li>
<li><code><a title="src.text.Embedding.LLM.huggingmodel.HuggingModel.embed" href="#src.text.Embedding.LLM.huggingmodel.HuggingModel.embed">embed</a></code></li>
<li><code><a title="src.text.Embedding.LLM.huggingmodel.HuggingModel.embed_tokenized" href="#src.text.Embedding.LLM.huggingmodel.HuggingModel.embed_tokenized">embed_tokenized</a></code></li>
<li><code><a title="src.text.Embedding.LLM.huggingmodel.HuggingModel.embed_words" href="#src.text.Embedding.LLM.huggingmodel.HuggingModel.embed_words">embed_words</a></code></li>
<li><code><a title="src.text.Embedding.LLM.huggingmodel.HuggingModel.fully_embed_tokenized" href="#src.text.Embedding.LLM.huggingmodel.HuggingModel.fully_embed_tokenized">fully_embed_tokenized</a></code></li>
<li><code><a title="src.text.Embedding.LLM.huggingmodel.HuggingModel.get_embedding_fun" href="#src.text.Embedding.LLM.huggingmodel.HuggingModel.get_embedding_fun">get_embedding_fun</a></code></li>
<li><code><a title="src.text.Embedding.LLM.huggingmodel.HuggingModel.get_prefix_mask" href="#src.text.Embedding.LLM.huggingmodel.HuggingModel.get_prefix_mask">get_prefix_mask</a></code></li>
<li><code><a title="src.text.Embedding.LLM.huggingmodel.HuggingModel.get_suffix_mask" href="#src.text.Embedding.LLM.huggingmodel.HuggingModel.get_suffix_mask">get_suffix_mask</a></code></li>
<li><code><a title="src.text.Embedding.LLM.huggingmodel.HuggingModel.max_token_length" href="#src.text.Embedding.LLM.huggingmodel.HuggingModel.max_token_length">max_token_length</a></code></li>
<li><code><a title="src.text.Embedding.LLM.huggingmodel.HuggingModel.max_word_length" href="#src.text.Embedding.LLM.huggingmodel.HuggingModel.max_word_length">max_word_length</a></code></li>
<li><code><a title="src.text.Embedding.LLM.huggingmodel.HuggingModel.model" href="#src.text.Embedding.LLM.huggingmodel.HuggingModel.model">model</a></code></li>
<li><code><a title="src.text.Embedding.LLM.huggingmodel.HuggingModel.padding_token" href="#src.text.Embedding.LLM.huggingmodel.HuggingModel.padding_token">padding_token</a></code></li>
<li><code><a title="src.text.Embedding.LLM.huggingmodel.HuggingModel.tokenize" href="#src.text.Embedding.LLM.huggingmodel.HuggingModel.tokenize">tokenize</a></code></li>
<li><code><a title="src.text.Embedding.LLM.huggingmodel.HuggingModel.tokenize_batch" href="#src.text.Embedding.LLM.huggingmodel.HuggingModel.tokenize_batch">tokenize_batch</a></code></li>
<li><code><a title="src.text.Embedding.LLM.huggingmodel.HuggingModel.tokenizer" href="#src.text.Embedding.LLM.huggingmodel.HuggingModel.tokenizer">tokenizer</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
