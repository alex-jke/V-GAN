<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>src.text.outlier_detection.space.token_space API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.text.outlier_detection.space.token_space</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.text.outlier_detection.space.token_space.TokenSpace"><code class="flex name class">
<span>class <span class="ident">TokenSpace</span></span>
<span>(</span><span>**params)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TokenSpace(Space):
    &#34;&#34;&#34;
    This class represents the token space that the outlier detection models should operate in.
    &#34;&#34;&#34;

    def __init__(self, **params):
        self.cache: Dict[str, PreparedData] = {}
        super().__init__(**params)

    @property
    def name(self):
        return &#34;Token-space&#34;

    def get_tokenized(self, dataset: Dataset, inlier_label) -&gt; Tuple[Tensor, Tensor, Tensor, Tensor]:
        _x_train, y_train = self._get_tokenized_with_labels(train=True, dataset=dataset, inlier_label=[inlier_label])
        _x_test, y_test = self._get_tokenized_with_labels(train=False, dataset=dataset,
                                                          inlier_label=dataset.get_possible_labels())

        train_length = _x_train.shape[1]
        test_length = _x_test.shape[1]

        pad_length = max(train_length, test_length)

        x_train = torch.nn.functional.pad(_x_train, (0, pad_length - train_length),
                                          value=self.model.padding_token).int()
        x_test = torch.nn.functional.pad(_x_test, (0, pad_length - test_length),
                                         value=self.model.padding_token).int()

        return x_train, y_train, x_test, y_test

    def embed_tokenized(self, tokenized: Tensor) -&gt; Tensor:
        embeddings = []
        assert len(tokenized.shape) == 2, f&#34;Tokenized data should be 2D (batch, sequence length), but got {tokenized.shape}&#34;
        with ui.display():
            for i in range(tokenized.shape[0]):
                ui.update(f&#34;Embedding {i}/{tokenized.shape[0]}&#34;)
                org_sample = tokenized[i]
                # Remove padding tokens
                sample = org_sample[org_sample != self.model.padding_token]


                assert len(sample.shape) == 1, f&#34;Sample does not contain the correct shape, expected: (token_length), got: {sample.shape}.&#34;
                if sample.shape[0] == 0:
                    sample = Tensor([self.model.padding_token]).to(sample.device)
                assert sample.shape[0] &gt; 0, f&#34;A sample needs to contain at least one non-padding token. Got: {sample}, sample with padding: {org_sample}.&#34;

                embedded_tokens = self.model.fully_embed_tokenized(sample) # TODO: normalize?
                embedding = embedded_tokens.mean(dim=0)
                #embedding = self.model.aggregateEmbeddings(embedded_tokens)
                assert len(embedding.shape) == 1, &#34;Embedding should be 1D.&#34;
                embeddings.append(embedding)
        return torch.stack(embeddings)

    def transform_dataset(self, dataset: Dataset, use_cached: bool, inlier_label, mask: Optional[ndarray]) -&gt; PreparedData:

        token_x_train, y_train, token_x_test, y_test = self.get_tokenized(dataset, inlier_label)

        id = dataset.name + str(use_cached) + str(inlier_label)
        if mask is None and use_cached and id in self.cache:
            return self.cache[id]

        if mask is not None and len(mask.shape) != 1:
            raise NotImplementedError(&#34;TokenSpace does not support masks with more than 1 dimension. Please use a mask of shape (sequence_length).&#34;)

        if mask is not None:
            token_x_train = token_x_train[:, :mask.shape[0]]
            token_x_test = token_x_test[:, :mask.shape[0]]

            assert mask.dtype == numpy.int64, f&#34;Mask should be of type int, but got {mask.dtype}&#34;

            bool_mask = (mask == 1)

            with ui.display():
                ui.update(&#34;train set&#34;)
                token_x_train = token_x_train[:,bool_mask]
                ui.update(&#34;test set&#34;)
                token_x_test = token_x_test[:,bool_mask]

        x_train = self.embed_tokenized(token_x_train)
        x_test = self.embed_tokenized(token_x_test)

        prepared_data = PreparedData(x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, space=self.name, inlier_labels=[inlier_label])

        if mask is None and use_cached:
            print(&#34;Cached&#34;)
            self.cache[id] = prepared_data

        return prepared_data

    def _get_tokenized_with_labels(self, train: bool, dataset: Dataset, inlier_label: list) -&gt; Tuple[Tensor, Tensor]:
        &#34;&#34;&#34;
        Tokenizes data and returns both tokenized data and corresponding labels.
        Handles both training and test cases.
        &#34;&#34;&#34;

        get_data = dataset.get_training_data if train else dataset.get_testing_data
        filtered_data, filtered_labels = self._process_data(get_data, inlier_label=inlier_label, amount=self.train_size if train else self.test_size)

        filtered_labels_tensor = Tensor(filtered_labels.tolist()).int().to(self.model.device)
        tokenized_data = self._filter_and_tokenize(filtered_data, self.train_size if train else self.test_size)
        if len(tokenized_data) != len(filtered_labels_tensor):
            raise ValueError(f&#34;The amount of labels and the amount of tokenized samples do not match. labels:  {len(filtered_labels_tensor)} != {len(tokenized_data)} (samples)&#34;)
        return tokenized_data, filtered_labels_tensor

    def _process_data(self, get_data: Callable[[], Tuple[pd.Series, pd.Series]], inlier_label: list, amount: int) -&gt; Tuple[pd.Series, pd.Series]:
        &#34;&#34;&#34;Filters training data to samples matching the inlier label used for training. Thus, the Dataset
        now only contains inliers.&#34;&#34;&#34;
        data, labels = get_data()
        filtered_data = data[labels.isin(inlier_label)] # Todo: add the discarded outliers to the test set
        selected_label = labels[labels.isin(inlier_label)]
        return filtered_data[:amount], selected_label[: amount]

    def _filter_and_tokenize(self, data: pd.Series, size: int) -&gt; Tensor:
        &#34;&#34;&#34;Truncates data to specified size and tokenizes it.&#34;&#34;&#34;
        length = min(size, len(data)) if size &gt; 0 else len(data)
        filtered_data = data[:length]
        tokenized = self.model.tokenize_batch(filtered_data.tolist())
        return tokenized</code></pre>
</details>
<div class="desc"><p>This class represents the token space that the outlier detection models should operate in.</p>
<p>Initializes the Space object.
:param model: The model to use to generate the space.
:param train_size: The size of the training set.
:param test_size: The size of the test set.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>text.outlier_detection.space.space.Space</li>
<li>abc.ABC</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="src.text.outlier_detection.space.token_space.TokenSpace.name"><code class="name">prop <span class="ident">name</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def name(self):
    return &#34;Token-space&#34;</code></pre>
</details>
<div class="desc"><p>Returns the name of the space.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="src.text.outlier_detection.space.token_space.TokenSpace.embed_tokenized"><code class="name flex">
<span>def <span class="ident">embed_tokenized</span></span>(<span>self, tokenized: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def embed_tokenized(self, tokenized: Tensor) -&gt; Tensor:
    embeddings = []
    assert len(tokenized.shape) == 2, f&#34;Tokenized data should be 2D (batch, sequence length), but got {tokenized.shape}&#34;
    with ui.display():
        for i in range(tokenized.shape[0]):
            ui.update(f&#34;Embedding {i}/{tokenized.shape[0]}&#34;)
            org_sample = tokenized[i]
            # Remove padding tokens
            sample = org_sample[org_sample != self.model.padding_token]


            assert len(sample.shape) == 1, f&#34;Sample does not contain the correct shape, expected: (token_length), got: {sample.shape}.&#34;
            if sample.shape[0] == 0:
                sample = Tensor([self.model.padding_token]).to(sample.device)
            assert sample.shape[0] &gt; 0, f&#34;A sample needs to contain at least one non-padding token. Got: {sample}, sample with padding: {org_sample}.&#34;

            embedded_tokens = self.model.fully_embed_tokenized(sample) # TODO: normalize?
            embedding = embedded_tokens.mean(dim=0)
            #embedding = self.model.aggregateEmbeddings(embedded_tokens)
            assert len(embedding.shape) == 1, &#34;Embedding should be 1D.&#34;
            embeddings.append(embedding)
    return torch.stack(embeddings)</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="src.text.outlier_detection.space.token_space.TokenSpace.get_tokenized"><code class="name flex">
<span>def <span class="ident">get_tokenized</span></span>(<span>self, dataset: text.dataset.dataset.Dataset, inlier_label) ‑> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_tokenized(self, dataset: Dataset, inlier_label) -&gt; Tuple[Tensor, Tensor, Tensor, Tensor]:
    _x_train, y_train = self._get_tokenized_with_labels(train=True, dataset=dataset, inlier_label=[inlier_label])
    _x_test, y_test = self._get_tokenized_with_labels(train=False, dataset=dataset,
                                                      inlier_label=dataset.get_possible_labels())

    train_length = _x_train.shape[1]
    test_length = _x_test.shape[1]

    pad_length = max(train_length, test_length)

    x_train = torch.nn.functional.pad(_x_train, (0, pad_length - train_length),
                                      value=self.model.padding_token).int()
    x_test = torch.nn.functional.pad(_x_test, (0, pad_length - test_length),
                                     value=self.model.padding_token).int()

    return x_train, y_train, x_test, y_test</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="src.text.outlier_detection.space.token_space.TokenSpace.transform_dataset"><code class="name flex">
<span>def <span class="ident">transform_dataset</span></span>(<span>self,<br>dataset: text.dataset.dataset.Dataset,<br>use_cached: bool,<br>inlier_label,<br>mask: numpy.ndarray | None) ‑> text.outlier_detection.space.prepared_data.PreparedData</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform_dataset(self, dataset: Dataset, use_cached: bool, inlier_label, mask: Optional[ndarray]) -&gt; PreparedData:

    token_x_train, y_train, token_x_test, y_test = self.get_tokenized(dataset, inlier_label)

    id = dataset.name + str(use_cached) + str(inlier_label)
    if mask is None and use_cached and id in self.cache:
        return self.cache[id]

    if mask is not None and len(mask.shape) != 1:
        raise NotImplementedError(&#34;TokenSpace does not support masks with more than 1 dimension. Please use a mask of shape (sequence_length).&#34;)

    if mask is not None:
        token_x_train = token_x_train[:, :mask.shape[0]]
        token_x_test = token_x_test[:, :mask.shape[0]]

        assert mask.dtype == numpy.int64, f&#34;Mask should be of type int, but got {mask.dtype}&#34;

        bool_mask = (mask == 1)

        with ui.display():
            ui.update(&#34;train set&#34;)
            token_x_train = token_x_train[:,bool_mask]
            ui.update(&#34;test set&#34;)
            token_x_test = token_x_test[:,bool_mask]

    x_train = self.embed_tokenized(token_x_train)
    x_test = self.embed_tokenized(token_x_test)

    prepared_data = PreparedData(x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test, space=self.name, inlier_labels=[inlier_label])

    if mask is None and use_cached:
        print(&#34;Cached&#34;)
        self.cache[id] = prepared_data

    return prepared_data</code></pre>
</details>
<div class="desc"><p>Prepares the data for the outlier detection model.
:param dataset: The dataset to prepare the data from.
:param use_cached: Whether to use cached data.
If True, the data will be loaded from cached files if it exists or
cached files will be created.
If False, the data will be created from scratch and not cached.
:param inlier_label: The label of the inliers.
:param mask: The mask to apply to the data. That is, the projection
into a subspace.
:return: The PreparedData object that contains the training and
testing data, projected in this space and then transformed into
the embedding space.
testing data.</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.text.outlier_detection.space" href="index.html">src.text.outlier_detection.space</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.text.outlier_detection.space.token_space.TokenSpace" href="#src.text.outlier_detection.space.token_space.TokenSpace">TokenSpace</a></code></h4>
<ul class="">
<li><code><a title="src.text.outlier_detection.space.token_space.TokenSpace.embed_tokenized" href="#src.text.outlier_detection.space.token_space.TokenSpace.embed_tokenized">embed_tokenized</a></code></li>
<li><code><a title="src.text.outlier_detection.space.token_space.TokenSpace.get_tokenized" href="#src.text.outlier_detection.space.token_space.TokenSpace.get_tokenized">get_tokenized</a></code></li>
<li><code><a title="src.text.outlier_detection.space.token_space.TokenSpace.name" href="#src.text.outlier_detection.space.token_space.TokenSpace.name">name</a></code></li>
<li><code><a title="src.text.outlier_detection.space.token_space.TokenSpace.transform_dataset" href="#src.text.outlier_detection.space.token_space.TokenSpace.transform_dataset">transform_dataset</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
