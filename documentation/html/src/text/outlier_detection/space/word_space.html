<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>src.text.outlier_detection.space.word_space API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.text.outlier_detection.space.word_space</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.text.outlier_detection.space.word_space.WordSpace"><code class="flex name class">
<span>class <span class="ident">WordSpace</span></span>
<span>(</span><span>strategy:Â text.Embedding.unification_strategy.UnificationStrategy, **params)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class WordSpace(Space):
    def __init__(self, strategy: UnificationStrategy, **params):
        self.strategy = strategy
        super().__init__(**params)
        self.cache: Dict[str, PreparedData] = {}

    def transform_dataset(self, dataset: AggregatableDataset, use_cached: bool, inlier_label, mask: Optional[ndarray] = None) -&gt; PreparedData:
        if self.strategy == UnificationStrategy.TRANSFORMER and not isinstance(dataset, AggregatableDataset):
            raise ValueError(&#34;WordSpace with transformer_aggregation set to True only works with AggregatableDataset.&#34;)

        id = dataset.name + str(use_cached) + str(inlier_label)
        if mask is None and use_cached and id in self.cache:
            return self.cache[id]
        if mask is not None:
            mask = Tensor(mask).to(self.model.device)

        preparer = DatasetPreparer(dataset, self.train_size)
        x_train, y_train = preparer.get_data_with_labels([inlier_label], train=True)
        preparer.max_samples = self.test_size
        x_test, y_test = preparer.get_data_with_labels(train=False)

        # Since we set aggregate to True, the word dimension is only of size 1,
        # but still present so that regardless of the aggregation method, the other models
        # can expect the same input shape.
        # avg_length = preparer.get_average_sentence_length(x_train)
        length = preparer.get_max_sentence_length(x_train)
        strategy_instance = self.strategy.create(length)
        embedded_train_with_word_dim = self.model.embed_sentences(x_train, dataset=dataset, strategy=strategy_instance, verbose=True, masks=mask)
        embedded_test_with_word_dim = self.model.embed_sentences(x_test, dataset=dataset, strategy=strategy_instance, verbose=True, masks=mask)

        if self.strategy == UnificationStrategy.TRANSFORMER or self.strategy == UnificationStrategy.MEAN:
            assert embedded_train_with_word_dim.shape[1] == 1, f&#34;expected shape (_, 1, _), got {embedded_train_with_word_dim.shape}&#34;
        embedded_train = embedded_train_with_word_dim.mean(dim=1)
        embedded_test = embedded_test_with_word_dim.mean(dim=1)

        y_train_int = y_train.astype(int)
        y_test_int = y_test.astype(int)

        y_train_tensor = Tensor(y_train_int.tolist()).int().to(self.model.device)
        y_test_tensor = Tensor(y_test_int.tolist()).int().to(self.model.device)

        prepared_data = PreparedData(x_train=embedded_train, y_train=y_train_tensor, x_test=embedded_test, y_test=y_test_tensor,
                            space=self.name, inlier_labels=[inlier_label])
        if mask is None and use_cached:
            print(&#34;Cached&#34;)
            self.cache[id] = prepared_data

        return prepared_data

    @property
    def name(self):
        return &#34;Word&#34; + &#34; &#34; + self.strategy.key

    def get_n_dims(self, x_train: ndarray) -&gt; int:
        &#34;&#34;&#34;
        Returns the number of dimensions of the space used as features.
        &#34;&#34;&#34;
        return VMMDTextLightningBase.get_average_sentence_length(x_train)</code></pre>
</details>
<div class="desc"><p>This class represents the space that the outlier detection models
should operate in.</p>
<p>Initializes the Space object.
:param model: The model to use to generate the space.
:param train_size: The size of the training set.
:param test_size: The size of the test set.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>text.outlier_detection.space.space.Space</li>
<li>abc.ABC</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="src.text.outlier_detection.space.word_space.WordSpace.name"><code class="name">prop <span class="ident">name</span></code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def name(self):
    return &#34;Word&#34; + &#34; &#34; + self.strategy.key</code></pre>
</details>
<div class="desc"><p>Returns the name of the space.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="src.text.outlier_detection.space.word_space.WordSpace.get_n_dims"><code class="name flex">
<span>def <span class="ident">get_n_dims</span></span>(<span>self, x_train:Â numpy.ndarray) â>Â int</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_n_dims(self, x_train: ndarray) -&gt; int:
    &#34;&#34;&#34;
    Returns the number of dimensions of the space used as features.
    &#34;&#34;&#34;
    return VMMDTextLightningBase.get_average_sentence_length(x_train)</code></pre>
</details>
<div class="desc"><p>Returns the number of dimensions of the space used as features.</p></div>
</dd>
<dt id="src.text.outlier_detection.space.word_space.WordSpace.transform_dataset"><code class="name flex">
<span>def <span class="ident">transform_dataset</span></span>(<span>self,<br>dataset:Â text.dataset.dataset.AggregatableDataset,<br>use_cached:Â bool,<br>inlier_label,<br>mask:Â numpy.ndarrayÂ |Â NoneÂ =Â None) â>Â text.outlier_detection.space.prepared_data.PreparedData</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform_dataset(self, dataset: AggregatableDataset, use_cached: bool, inlier_label, mask: Optional[ndarray] = None) -&gt; PreparedData:
    if self.strategy == UnificationStrategy.TRANSFORMER and not isinstance(dataset, AggregatableDataset):
        raise ValueError(&#34;WordSpace with transformer_aggregation set to True only works with AggregatableDataset.&#34;)

    id = dataset.name + str(use_cached) + str(inlier_label)
    if mask is None and use_cached and id in self.cache:
        return self.cache[id]
    if mask is not None:
        mask = Tensor(mask).to(self.model.device)

    preparer = DatasetPreparer(dataset, self.train_size)
    x_train, y_train = preparer.get_data_with_labels([inlier_label], train=True)
    preparer.max_samples = self.test_size
    x_test, y_test = preparer.get_data_with_labels(train=False)

    # Since we set aggregate to True, the word dimension is only of size 1,
    # but still present so that regardless of the aggregation method, the other models
    # can expect the same input shape.
    # avg_length = preparer.get_average_sentence_length(x_train)
    length = preparer.get_max_sentence_length(x_train)
    strategy_instance = self.strategy.create(length)
    embedded_train_with_word_dim = self.model.embed_sentences(x_train, dataset=dataset, strategy=strategy_instance, verbose=True, masks=mask)
    embedded_test_with_word_dim = self.model.embed_sentences(x_test, dataset=dataset, strategy=strategy_instance, verbose=True, masks=mask)

    if self.strategy == UnificationStrategy.TRANSFORMER or self.strategy == UnificationStrategy.MEAN:
        assert embedded_train_with_word_dim.shape[1] == 1, f&#34;expected shape (_, 1, _), got {embedded_train_with_word_dim.shape}&#34;
    embedded_train = embedded_train_with_word_dim.mean(dim=1)
    embedded_test = embedded_test_with_word_dim.mean(dim=1)

    y_train_int = y_train.astype(int)
    y_test_int = y_test.astype(int)

    y_train_tensor = Tensor(y_train_int.tolist()).int().to(self.model.device)
    y_test_tensor = Tensor(y_test_int.tolist()).int().to(self.model.device)

    prepared_data = PreparedData(x_train=embedded_train, y_train=y_train_tensor, x_test=embedded_test, y_test=y_test_tensor,
                        space=self.name, inlier_labels=[inlier_label])
    if mask is None and use_cached:
        print(&#34;Cached&#34;)
        self.cache[id] = prepared_data

    return prepared_data</code></pre>
</details>
<div class="desc"><p>Prepares the data for the outlier detection model.
:param dataset: The dataset to prepare the data from.
:param use_cached: Whether to use cached data.
If True, the data will be loaded from cached files if it exists or
cached files will be created.
If False, the data will be created from scratch and not cached.
:param inlier_label: The label of the inliers.
:param mask: The mask to apply to the data. That is, the projection
into a subspace.
:return: The PreparedData object that contains the training and
testing data, projected in this space and then transformed into
the embedding space.
testing data.</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.text.outlier_detection.space" href="index.html">src.text.outlier_detection.space</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.text.outlier_detection.space.word_space.WordSpace" href="#src.text.outlier_detection.space.word_space.WordSpace">WordSpace</a></code></h4>
<ul class="">
<li><code><a title="src.text.outlier_detection.space.word_space.WordSpace.get_n_dims" href="#src.text.outlier_detection.space.word_space.WordSpace.get_n_dims">get_n_dims</a></code></li>
<li><code><a title="src.text.outlier_detection.space.word_space.WordSpace.name" href="#src.text.outlier_detection.space.word_space.WordSpace.name">name</a></code></li>
<li><code><a title="src.text.outlier_detection.space.word_space.WordSpace.transform_dataset" href="#src.text.outlier_detection.space.word_space.WordSpace.transform_dataset">transform_dataset</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
