<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>src.text.vmmd_lightning_text_experiments API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.text.vmmd_lightning_text_experiments</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="src.text.vmmd_lightning_text_experiments.VMMDLightningTextExperiment"><code class="flex name class">
<span>class <span class="ident">VMMDLightningTextExperiment</span></span>
<span>(</span><span>emb_model: text.Embedding.LLM.huggingmodel.HuggingModel,<br>vmmd_model: Type[modules.text.vmmd_text_lightning.VMMDTextLightningBase],<br>generator: Type[models.Generator.Generator_big],<br>version: str,<br>dataset: text.dataset.dataset.AggregatableDataset,<br>samples: int,<br>yield_epochs: int,<br>lr: float,<br>weight_decay: float,<br>penalty_weight: float,<br>batch_size: int,<br>epochs: int,<br>export_path: pathlib.Path | None = None,<br>export: bool | None = True,<br>sequence_length: int | None = None,<br>aggregation_strategy: text.Embedding.unification_strategy.StrategyInstance = TRANSFORMER,<br>train_flag: bool = True,<br>use_mmd: bool = False,<br>labels: list | None = None,<br>apply_gradient_clipping=True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VMMDLightningTextExperiment:
    &#34;&#34;&#34;
    A class to run VMMD experiments with PyTorch Lightning on Text data.
    This class is designed to be used with the subclasses of VMMDTextLightningBase.
    It handles the training and visualization of the model.
    &#34;&#34;&#34;
    def __init__(self,
                 emb_model: HuggingModel,
                 vmmd_model: Type[VMMDTextLightningBase],
                 generator: Type[Generator_big],
                 version: str,
                 dataset: AggregatableDataset,
                 samples: int,
                 yield_epochs: int,
                 lr: float,
                 weight_decay: float,
                 penalty_weight: float,
                 batch_size: int,
                 epochs: int,
                 export_path: Optional[Path] = None,
                 export: Optional[bool] = True,
                 sequence_length: Optional[int] = None,
                 aggregation_strategy: StrategyInstance = UnificationStrategy.TRANSFORMER.create(),
                 train_flag: bool = True,
                 use_mmd: bool = False,
                 labels: Optional[list] = None,
                 apply_gradient_clipping = True):
        self.emb_model = emb_model
        self.generator = generator
        self.version = version
        self.dataset = dataset
        self.samples = samples
        self.yield_epochs = yield_epochs
        self.lr = lr
        self.weight_decay = weight_decay
        self.penalty_weight = penalty_weight
        self.batch_size = batch_size
        self.epochs = epochs
        self.sequence_length = sequence_length
        self.strategy = aggregation_strategy
        self.train_flag = train_flag
        self.vmmd_model = vmmd_model
        self.seperator = &#34; &#34;
        self.export_path = export_path
        self.export = export
        self.loaded_model = False
        self.use_mmd = use_mmd
        self.labels = labels
        self.apply_gradient_clipping = apply_gradient_clipping
        if self.export_path is None and export:
            self.export_path = self.build_export_path()

    def build_export_path(self) -&gt; Path:
        base_dir = Path(os.path.join(
            os.getcwd(),
            &#39;experiments&#39;,
            self.vmmd_model.__name__,
            self.emb_model.__class__.__name__,
            self.generator.__name__,
            f&#34;{self.version}&#34;,
            self.strategy.key(),
            f&#34;{self.dataset.name}_sl{self.sequence_length}_s{self.samples}&#34;
        ))
        if self.train_flag:
            base_dir = Path(str(base_dir) + (&#34;_&#34; + datetime.now().strftime(&#34;%Y%m%d-%H%M%S&#34;)))
        return base_dir

    def visualize(self, epoch: int, model, sentences: ndarray):
        samples = 30
        tokenized_sentences: List[List[str]] = [
            self.emb_model.get_words(sentence) for sentence in sentences[:samples]
        ]
        visualizer = CollectiveVisualizer(
            tokenized_data=tokenized_sentences,
            tokenizer=None,
            vmmd_model=model,
            export_path=str(self.export_path),
            text_visualization=True
        )
        visualizer.visualize(epoch=epoch, samples=samples)
        model._export(model.generator, export_params=False, x_data=sentences)

    def _get_average_sentence_length(self, x_data: ndarray[str]) -&gt; int:
        &#34;&#34;&#34;
        Calculate the average sentence length in the dataset.
        :param x_data: A numpy array of sentences.
        :return: The average sentence length, as an integer.
        &#34;&#34;&#34;
        sequence_length = int(np.mean([len(x.split(self.seperator)) for x in x_data]))
        return sequence_length

    def _prepare_data(self) -&gt; ndarray[str]:
        &#34;&#34;&#34;
        Prepare the data for training.
        :param model: The VMMDTextLightningBase model to use.
        :return: The training data, as a numpy array of sentences.
        &#34;&#34;&#34;
        # Prepare training data using a DatasetPreparer.
        preparer = DatasetPreparer(self.dataset, max_samples=self.samples)
        _x_train = preparer.get_training_data(labels=self.labels)

        # Get the average sentence length from the dataset.
        if self.sequence_length is None:
            self.sequence_length = self._get_average_sentence_length(_x_train)

        return _x_train

    def _prepare_vmmd_model(self, embedding_fun: Callable) -&gt; VMMDTextLightningBase:
        &#34;&#34;&#34;
        Prepare the VMMD model for training.
        &#34;&#34;&#34;
        return self.vmmd_model(
            embedding=embedding_fun,
            sequence_length=self.sequence_length,
            lr=self.lr,
            weight_decay=self.weight_decay,
            weight=self.penalty_weight,
            generator=self.generator,
            seed=777,
            strategy=self.strategy,
            use_mmd=self.use_mmd,
            batch_size=self.batch_size, # This is not needed for the training itself, just for the params file.
            epochs=self.epochs, # This is also not needed for the training itself, just for the params file.
        )

    def _load_if_exists(self, model: VMMDTextLightningBase, embedding_fun) -&gt; VMMDTextLightningBase:
        &#34;&#34;&#34;
        Load the model if it exists, otherwise return the model.
        :param model: The VMMDTextLightningBase model to load.
        :return: The loaded model, or the original model if it does not exist.
        &#34;&#34;&#34;
        if self.export_path is not None and self.export_path.exists():
            print(f&#34;Loading model from {self.export_path}&#34;)
            ckpt_path_dir = self.export_path / &#34;tensorboard_logs&#34; / &#34;version_0&#34; / &#34;checkpoints&#34;
            ckpt_path_list = list(ckpt_path_dir.glob(&#34;*.ckpt&#34;))
            ckpt_path = ckpt_path_list[-1] if len(ckpt_path_list) &gt; 0 else None
            if ckpt_path is None:
                #raise FileNotFoundError(f&#34;Checkpoint not found in {ckpt_path_dir}.&#34;)
                print(&#34;No checkpoint found. This is unexpected behavior. If the model path exists a ckpt file should also exist.&#34;)
                return model
            model = VMMDTextLightning.load_from_checkpoint(checkpoint_path=ckpt_path)

            # The embedding function is not saved in the checkpoint, so it needs to be set it again.
            model.embedding = embedding_fun
            self.loaded_model = True
        return model

    def run(self)  -&gt; VMMDTextLightningBase:
        &#34;&#34;&#34;
        Run the VMMD experiment, including training and visualization.
        The parameters for the experiment are set in the constructor.
        The training data is prepared using the DatasetPreparer class.
        The model is trained using PyTorch Lightning.
        The visualization is done using the CollectiveVisualizer class.
        :return: The trained VMMD model.
        &#34;&#34;&#34;
        # Instantiate the model with the provided hyperparameters.
        _x_train = self._prepare_data()

        #strategy = UnificationStrategy.TRANSFORMER.create() if self.transformer_aggregation else UnificationStrategy.MEAN.create()

        embedding_fun = lambda samples, padding_length, masks: self.emb_model.embed_sentences(sentences=samples,
                                                                                         masks=masks,
                                                                                         strategy = self.strategy,
                                                                                         dataset=self.dataset)
        model = self._prepare_vmmd_model(embedding_fun)

        # Load the model if it exists.
        model = self._load_if_exists(model, embedding_fun)
        if self.loaded_model:
            print(&#34;Model loaded successfully.&#34;)
            return model

        x_train = model.get_training_data(_x_train, embedding_fun, None)

        data_loader = DataLoader(np.array(x_train), batch_size=self.batch_size,
                                 #drop_last=True, pin_memory=True, shuffle=True, num_workers=10, persistent_workers=True
                                 )

        loggers = []
        callbacks = []
        if self.export:
            # Set up the visualization callback.
            vis_cb = VisualizationCallback(
                emb_model=self.emb_model,
                export_path=str(self.export_path),
                dataset=self.dataset,
                yield_epochs=self.yield_epochs,
                samples=30
            )

            tensorboard_logger = TensorBoardLogger(
                save_dir=self.export_path,
                name=&#34;tensorboard_logs&#34;
            )
            print(&#34;TensorBoard Logs at: &#34;, self.export_path  / &#34;tensorboard_logs&#34;)

            csv_logger = CSVLogger(
                save_dir=self.export_path,
                name=&#34;lightning_logs&#34;,
                version=0
            )
            loggers = [tensorboard_logger, csv_logger]
            callbacks = [vis_cb]

        trainer = Trainer(
            max_epochs=self.epochs,
            callbacks=callbacks,
            default_root_dir=self.export_path,
            log_every_n_steps=1, # Log every step, as the visualizer loads the csv file created by the logger.
            accelerator=&#34;auto&#34;,
            logger=loggers,
            gradient_clip_val=0.5 if self.apply_gradient_clipping else 0,
            devices=1
        )
        # Start training.
        trainer.fit(model, train_dataloaders=data_loader)

        if self.export:
            self.visualize(epoch=self.epochs, model=model, sentences=np.array(x_train))
        return model</code></pre>
</details>
<div class="desc"><p>A class to run VMMD experiments with PyTorch Lightning on Text data.
This class is designed to be used with the subclasses of VMMDTextLightningBase.
It handles the training and visualization of the model.</p></div>
<h3>Methods</h3>
<dl>
<dt id="src.text.vmmd_lightning_text_experiments.VMMDLightningTextExperiment.build_export_path"><code class="name flex">
<span>def <span class="ident">build_export_path</span></span>(<span>self) ‑> pathlib.Path</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def build_export_path(self) -&gt; Path:
    base_dir = Path(os.path.join(
        os.getcwd(),
        &#39;experiments&#39;,
        self.vmmd_model.__name__,
        self.emb_model.__class__.__name__,
        self.generator.__name__,
        f&#34;{self.version}&#34;,
        self.strategy.key(),
        f&#34;{self.dataset.name}_sl{self.sequence_length}_s{self.samples}&#34;
    ))
    if self.train_flag:
        base_dir = Path(str(base_dir) + (&#34;_&#34; + datetime.now().strftime(&#34;%Y%m%d-%H%M%S&#34;)))
    return base_dir</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="src.text.vmmd_lightning_text_experiments.VMMDLightningTextExperiment.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self) ‑> modules.text.vmmd_text_lightning.VMMDTextLightningBase</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self)  -&gt; VMMDTextLightningBase:
    &#34;&#34;&#34;
    Run the VMMD experiment, including training and visualization.
    The parameters for the experiment are set in the constructor.
    The training data is prepared using the DatasetPreparer class.
    The model is trained using PyTorch Lightning.
    The visualization is done using the CollectiveVisualizer class.
    :return: The trained VMMD model.
    &#34;&#34;&#34;
    # Instantiate the model with the provided hyperparameters.
    _x_train = self._prepare_data()

    #strategy = UnificationStrategy.TRANSFORMER.create() if self.transformer_aggregation else UnificationStrategy.MEAN.create()

    embedding_fun = lambda samples, padding_length, masks: self.emb_model.embed_sentences(sentences=samples,
                                                                                     masks=masks,
                                                                                     strategy = self.strategy,
                                                                                     dataset=self.dataset)
    model = self._prepare_vmmd_model(embedding_fun)

    # Load the model if it exists.
    model = self._load_if_exists(model, embedding_fun)
    if self.loaded_model:
        print(&#34;Model loaded successfully.&#34;)
        return model

    x_train = model.get_training_data(_x_train, embedding_fun, None)

    data_loader = DataLoader(np.array(x_train), batch_size=self.batch_size,
                             #drop_last=True, pin_memory=True, shuffle=True, num_workers=10, persistent_workers=True
                             )

    loggers = []
    callbacks = []
    if self.export:
        # Set up the visualization callback.
        vis_cb = VisualizationCallback(
            emb_model=self.emb_model,
            export_path=str(self.export_path),
            dataset=self.dataset,
            yield_epochs=self.yield_epochs,
            samples=30
        )

        tensorboard_logger = TensorBoardLogger(
            save_dir=self.export_path,
            name=&#34;tensorboard_logs&#34;
        )
        print(&#34;TensorBoard Logs at: &#34;, self.export_path  / &#34;tensorboard_logs&#34;)

        csv_logger = CSVLogger(
            save_dir=self.export_path,
            name=&#34;lightning_logs&#34;,
            version=0
        )
        loggers = [tensorboard_logger, csv_logger]
        callbacks = [vis_cb]

    trainer = Trainer(
        max_epochs=self.epochs,
        callbacks=callbacks,
        default_root_dir=self.export_path,
        log_every_n_steps=1, # Log every step, as the visualizer loads the csv file created by the logger.
        accelerator=&#34;auto&#34;,
        logger=loggers,
        gradient_clip_val=0.5 if self.apply_gradient_clipping else 0,
        devices=1
    )
    # Start training.
    trainer.fit(model, train_dataloaders=data_loader)

    if self.export:
        self.visualize(epoch=self.epochs, model=model, sentences=np.array(x_train))
    return model</code></pre>
</details>
<div class="desc"><p>Run the VMMD experiment, including training and visualization.
The parameters for the experiment are set in the constructor.
The training data is prepared using the DatasetPreparer class.
The model is trained using PyTorch Lightning.
The visualization is done using the CollectiveVisualizer class.
:return: The trained VMMD model.</p></div>
</dd>
<dt id="src.text.vmmd_lightning_text_experiments.VMMDLightningTextExperiment.visualize"><code class="name flex">
<span>def <span class="ident">visualize</span></span>(<span>self, epoch: int, model, sentences: numpy.ndarray)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def visualize(self, epoch: int, model, sentences: ndarray):
    samples = 30
    tokenized_sentences: List[List[str]] = [
        self.emb_model.get_words(sentence) for sentence in sentences[:samples]
    ]
    visualizer = CollectiveVisualizer(
        tokenized_data=tokenized_sentences,
        tokenizer=None,
        vmmd_model=model,
        export_path=str(self.export_path),
        text_visualization=True
    )
    visualizer.visualize(epoch=epoch, samples=samples)
    model._export(model.generator, export_params=False, x_data=sentences)</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.text" href="index.html">src.text</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="src.text.vmmd_lightning_text_experiments.VMMDLightningTextExperiment" href="#src.text.vmmd_lightning_text_experiments.VMMDLightningTextExperiment">VMMDLightningTextExperiment</a></code></h4>
<ul class="">
<li><code><a title="src.text.vmmd_lightning_text_experiments.VMMDLightningTextExperiment.build_export_path" href="#src.text.vmmd_lightning_text_experiments.VMMDLightningTextExperiment.build_export_path">build_export_path</a></code></li>
<li><code><a title="src.text.vmmd_lightning_text_experiments.VMMDLightningTextExperiment.run" href="#src.text.vmmd_lightning_text_experiments.VMMDLightningTextExperiment.run">run</a></code></li>
<li><code><a title="src.text.vmmd_lightning_text_experiments.VMMDLightningTextExperiment.visualize" href="#src.text.vmmd_lightning_text_experiments.VMMDLightningTextExperiment.visualize">visualize</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
